DEBUG:root:**info                              [a100-5h]
DEBUG:root:seed                                [42]
DEBUG:root:model                               [meta-llama/Llama-2-13b-hf]
DEBUG:root:model_short                         [llama-13b-hf]
DEBUG:root:mode_inference                      [single]
DEBUG:root:mode_conclusion                     [single]
DEBUG:root:nr_prompt_inference                 [0]
DEBUG:root:nr_prompt_conclusion                [0]
DEBUG:root:n_step                              [1]
DEBUG:root:arg dataset_name                    [proofwriter_6000]
DEBUG:root:arg data_file_name                  [CWA_REAL_depth-5_1000]
DEBUG:root:torch_dtype                         [torch.float16]
DEBUG:root:dataset_file_full_path              [/dss/dsshome1/0A/di35fer/dataset/proofwriter_6000/CWA_REAL_depth-5_1000.jsonl]
DEBUG:root:path_output_log                     [/dss/dsshome1/0A/di35fer/code/code_vanilla/result/module/proofwriter_6000/NL_inf[single]_conc[single]_[1]_[llama-13b-hf]_pinf[0]_pconc[0]_42_CWA_REAL_depth-5_1000_02_05_19_22_28_a100-5h_log.txt]
DEBUG:root:output_file_path                    [/dss/dsshome1/0A/di35fer/code/code_vanilla/result/module/proofwriter_6000/NL_inf[single]_conc[single]_[1]_[llama-13b-hf]_pinf[0]_pconc[0]_42_CWA_REAL_depth-5_1000_02_05_19_22_28_a100-5h_res.csv]
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /meta-llama/Llama-2-13b-hf/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /meta-llama/Llama-2-13b-hf/resolve/main/config.json HTTP/1.1" 200 0
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  2.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.04s/it]
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /meta-llama/Llama-2-13b-hf/resolve/main/generation_config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/json/json.py HTTP/1.1" 200 0
DEBUG:fsspec.local:open file: /dss/dsshome1/0A/di35fer/.cache/huggingface/datasets/json/default-21746a836908a62a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json
DEBUG:fsspec.local:open file: /dss/dsshome1/0A/di35fer/.cache/huggingface/datasets/json/default-21746a836908a62a/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json
  0%|          | 0/1000 [00:00<?, ?it/s]/transformers/src/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/transformers/src/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
  0%|          | 1/1000 [04:12<69:56:52, 252.06s/it]  0%|          | 2/1000 [08:36<71:52:06, 259.25s/it]  0%|          | 3/1000 [12:59<72:18:31, 261.09s/it]  0%|          | 4/1000 [16:57<69:40:54, 251.86s/it]  0%|          | 5/1000 [21:33<72:01:33, 260.60s/it]  1%|          | 6/1000 [26:16<74:01:56, 268.13s/it]  1%|          | 7/1000 [30:40<73:38:15, 266.96s/it]  1%|          | 8/1000 [35:18<74:29:05, 270.31s/it]  1%|          | 9/1000 [39:03<70:29:36, 256.08s/it]  1%|          | 10/1000 [43:13<69:58:03, 254.43s/it]  1%|          | 11/1000 [47:25<69:41:51, 253.70s/it]  1%|          | 12/1000 [51:40<69:42:46, 254.01s/it]  1%|▏         | 13/1000 [56:23<72:03:24, 262.82s/it]  1%|▏         | 14/1000 [1:01:00<73:11:06, 267.21s/it]  2%|▏         | 15/1000 [1:05:24<72:47:45, 266.06s/it]  2%|▏         | 16/1000 [1:09:52<72:54:43, 266.75s/it]  2%|▏         | 17/1000 [1:13:23<68:15:46, 250.00s/it]  2%|▏         | 18/1000 [1:17:34<68:13:57, 250.14s/it]  2%|▏         | 19/1000 [1:22:15<70:43:02, 259.51s/it]  2%|▏         | 20/1000 [1:26:52<72:05:26, 264.82s/it]  2%|▏         | 21/1000 [1:31:44<74:11:21, 272.81s/it]  2%|▏         | 22/1000 [1:36:08<73:25:55, 270.30s/it]  2%|▏         | 23/1000 [1:40:50<74:18:15, 273.79s/it]  2%|▏         | 24/1000 [1:45:30<74:43:29, 275.62s/it]  2%|▎         | 25/1000 [1:50:09<74:53:12, 276.51s/it]  3%|▎         | 26/1000 [1:54:48<75:01:42, 277.31s/it]  3%|▎         | 27/1000 [1:59:26<75:00:11, 277.50s/it]  3%|▎         | 28/1000 [2:04:30<77:04:02, 285.43s/it]  3%|▎         | 29/1000 [2:08:54<75:19:28, 279.27s/it]  3%|▎         | 30/1000 [2:13:32<75:07:23, 278.81s/it]  3%|▎         | 31/1000 [2:17:58<73:58:58, 274.86s/it]  3%|▎         | 32/1000 [2:22:49<75:13:55, 279.79s/it]  3%|▎         | 33/1000 [2:27:28<75:06:38, 279.63s/it]  3%|▎         | 34/1000 [2:32:21<76:05:17, 283.56s/it]  4%|▎         | 35/1000 [2:36:34<73:32:26, 274.35s/it]  4%|▎         | 36/1000 [2:40:32<70:30:37, 263.32s/it]  4%|▎         | 37/1000 [2:45:36<73:43:54, 275.63s/it]  4%|▍         | 38/1000 [2:50:06<73:14:37, 274.09s/it]  4%|▍         | 39/1000 [2:54:18<71:20:20, 267.24s/it]  4%|▍         | 40/1000 [2:59:10<73:17:37, 274.85s/it]  4%|▍         | 41/1000 [3:03:35<72:26:33, 271.94s/it]  4%|▍         | 42/1000 [3:08:05<72:10:27, 271.22s/it]  4%|▍         | 43/1000 [3:12:30<71:36:54, 269.40s/it]  4%|▍         | 44/1000 [3:17:22<73:18:13, 276.04s/it]  4%|▍         | 45/1000 [3:22:00<73:22:36, 276.60s/it]  5%|▍         | 46/1000 [3:26:10<71:11:34, 268.65s/it]  5%|▍         | 47/1000 [3:30:37<71:02:03, 268.33s/it]  5%|▍         | 48/1000 [3:34:36<68:37:38, 259.51s/it]  5%|▍         | 49/1000 [3:38:54<68:25:08, 259.00s/it]  5%|▌         | 50/1000 [3:43:43<70:41:08, 267.86s/it]  5%|▌         | 51/1000 [3:48:22<71:30:08, 271.24s/it]  5%|▌         | 52/1000 [3:52:47<70:55:46, 269.35s/it]  5%|▌         | 53/1000 [3:57:12<70:30:58, 268.07s/it]  5%|▌         | 54/1000 [4:01:37<70:12:06, 267.15s/itsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.
]  6%|▌         | 55/1000 [4:06:29<72:06:12, 274.68s/it]  6%|▌         | 56/1000 [4:10:54<71:17:26, 271.87s/it]  6%|▌         | 57/1000 [4:15:47<72:52:05, 278.18s/it]  6%|▌         | 58/1000 [4:20:52<74:53:11, 286.19s/it]  6%|▌         | 59/1000 [4:25:17<73:09:52, 279.91s/it]  6%|▌         | 60/1000 [4:29:17<69:54:38, 267.74s/it]  6%|▌         | 61/1000 [4:33:57<70:47:09, 271.38s/it]  6%|▌         | 62/1000 [4:38:22<70:16:41, 269.72s/it]  6%|▋         | 63/1000 [4:42:53<70:17:37, 270.07s/it]  6%|▋         | 64/1000 [4:47:32<70:51:27, 272.53s/it]  6%|▋         | 65/1000 [4:52:38<73:23:50, 282.60s/it]  7%|▋         | 66/1000 [4:57:35<74:27:20, 286.98s/it]slurmstepd: error: *** STEP 276694.0 ON lrz-dgx-a100-005 CANCELLED AT 2024-02-06T00:22:19 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 276694 ON lrz-dgx-a100-005 CANCELLED AT 2024-02-06T00:22:19 DUE TO TIME LIMIT ***
