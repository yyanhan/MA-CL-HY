DEBUG:root:**info                              [a100-5h]
DEBUG:root:seed                                [42]
DEBUG:root:model                               [meta-llama/Llama-2-13b-hf]
DEBUG:root:model_short                         [llama-13b-hf]
DEBUG:root:mode_inference                      [single]
DEBUG:root:mode_conclusion                     [single]
DEBUG:root:nr_prompt_inference                 [0]
DEBUG:root:nr_prompt_conclusion                [0]
DEBUG:root:n_step                              [1]
DEBUG:root:arg dataset_name                    [proofwriter_6000]
DEBUG:root:arg data_file_name                  [CWA_REAL_depth-4_1000]
DEBUG:root:torch_dtype                         [torch.float16]
DEBUG:root:dataset_file_full_path              [/dss/dsshome1/0A/di35fer/dataset/proofwriter_6000/CWA_REAL_depth-4_1000.jsonl]
DEBUG:root:path_output_log                     [/dss/dsshome1/0A/di35fer/code/code_vanilla/result/module/proofwriter_6000/NL_inf[single]_conc[single]_[1]_[llama-13b-hf]_pinf[0]_pconc[0]_42_CWA_REAL_depth-4_1000_02_05_18_56_03_a100-5h_log.txt]
DEBUG:root:output_file_path                    [/dss/dsshome1/0A/di35fer/code/code_vanilla/result/module/proofwriter_6000/NL_inf[single]_conc[single]_[1]_[llama-13b-hf]_pinf[0]_pconc[0]_42_CWA_REAL_depth-4_1000_02_05_18_56_03_a100-5h_res.csv]
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /meta-llama/Llama-2-13b-hf/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /meta-llama/Llama-2-13b-hf/resolve/main/config.json HTTP/1.1" 200 0
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.86s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.54s/it]
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /meta-llama/Llama-2-13b-hf/resolve/main/generation_config.json HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/json/json.py HTTP/1.1" 200 0
DEBUG:fsspec.local:open file: /dss/dsshome1/0A/di35fer/.cache/huggingface/datasets/json/default-c1608136d1e096e8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json
DEBUG:fsspec.local:open file: /dss/dsshome1/0A/di35fer/.cache/huggingface/datasets/json/default-c1608136d1e096e8/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/dataset_info.json
  0%|          | 0/1000 [00:00<?, ?it/s]/transformers/src/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/transformers/src/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
  0%|          | 1/1000 [04:30<74:58:56, 270.21s/it]  0%|          | 2/1000 [08:53<73:44:44, 266.02s/it]  0%|          | 3/1000 [13:54<78:06:32, 282.04s/it]  0%|          | 4/1000 [18:41<78:37:06, 284.16s/it]  0%|          | 5/1000 [23:37<79:44:03, 288.49s/it]  1%|          | 6/1000 [28:12<78:19:41, 283.68s/it]  1%|          | 7/1000 [32:46<77:25:40, 280.70s/it]  1%|          | 8/1000 [37:44<78:52:01, 286.21s/it]  1%|          | 9/1000 [42:13<77:18:41, 280.85s/it]  1%|          | 10/1000 [46:22<74:27:30, 270.76s/it]  1%|          | 11/1000 [51:18<76:29:48, 278.45s/it]  1%|          | 12/1000 [55:33<74:31:39, 271.56s/it]  1%|▏         | 13/1000 [1:00:04<74:24:48, 271.42s/it]  1%|▏         | 14/1000 [1:04:38<74:29:16, 271.96s/it]  2%|▏         | 15/1000 [1:09:37<76:42:03, 280.33s/it]  2%|▏         | 16/1000 [1:14:24<77:07:41, 282.18s/it]  2%|▏         | 17/1000 [1:18:30<74:05:47, 271.36s/it]  2%|▏         | 18/1000 [1:23:03<74:08:36, 271.81s/it]  2%|▏         | 19/1000 [1:28:03<76:21:46, 280.23s/it]  2%|▏         | 20/1000 [1:32:36<75:41:56, 278.08s/it]  2%|▏         | 21/1000 [1:37:24<76:28:30, 281.22s/it]  2%|▏         | 22/1000 [1:42:11<76:51:24, 282.91s/it]  2%|▏         | 23/1000 [1:46:03<72:34:41, 267.43s/it]  2%|▏         | 24/1000 [1:50:21<71:44:51, 264.64s/it]  2%|▎         | 25/1000 [1:54:52<72:13:02, 266.65s/it]  3%|▎         | 26/1000 [1:59:37<73:39:46, 272.27s/it]  3%|▎         | 27/1000 [2:04:08<73:27:02, 271.76s/it]  3%|▎         | 28/1000 [2:08:52<74:24:48, 275.61s/it]  3%|▎         | 29/1000 [2:13:16<73:20:03, 271.89s/it]  3%|▎         | 30/1000 [2:17:39<72:32:01, 269.20s/it]  3%|▎         | 31/1000 [2:22:36<74:43:23, 277.61s/it]  3%|▎         | 32/1000 [2:27:06<74:03:04, 275.40s/it]  3%|▎         | 33/1000 [2:31:37<73:39:06, 274.19s/it]  3%|▎         | 34/1000 [2:36:14<73:45:52, 274.90s/it]  4%|▎         | 35/1000 [2:41:16<75:53:55, 283.15s/it]  4%|▎         | 36/1000 [2:46:00<75:51:01, 283.26s/it]  4%|▎         | 37/1000 [2:50:56<76:46:46, 287.03s/it]  4%|▍         | 38/1000 [2:55:00<73:15:36, 274.15s/it]  4%|▍         | 39/1000 [2:59:24<72:21:34, 271.07s/it]  4%|▍         | 40/1000 [3:03:41<71:10:23, 266.90s/it]  4%|▍         | 41/1000 [3:08:25<72:30:05, 272.16s/it]  4%|▍         | 42/1000 [3:13:02<72:47:20, 273.53s/it]  4%|▍         | 43/1000 [3:17:41<73:06:40, 275.03s/it]  4%|▍         | 44/1000 [3:22:14<72:51:59, 274.39s/it]  4%|▍         | 45/1000 [3:26:19<70:31:26, 265.85s/it]  5%|▍         | 46/1000 [3:30:53<71:02:32, 268.08s/it]  5%|▍         | 47/1000 [3:35:28<71:32:49, 270.27s/it]  5%|▍         | 48/1000 [3:40:13<72:36:05, 274.54s/it]  5%|▍         | 49/1000 [3:45:10<74:18:00, 281.26s/it]  5%|▌         | 50/1000 [3:50:21<76:36:15, 290.29s/it]  5%|▌         | 51/1000 [3:55:05<75:59:53, 288.30s/it]  5%|▌         | 52/1000 [3:59:52<75:52:02, 288.10s/it]  5%|▌         | 53/1000 [4:04:38<75:34:43, 287.31s/it]  5%|▌         | 54/1000 [4:09:25<75:29:02, 287.25s/srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 276693 ON lrz-dgx-a100-005 CANCELLED AT 2024-02-05T23:55:49 DUE TO TIME LIMIT ***
it]  6%|▌         | 55/1000 [4:14:25<76:25:32, 291.15s/it]  6%|▌         | 56/1000 [4:19:03<75:17:56, 287.16s/it]  6%|▌         | 57/1000 [4:23:53<75:28:25, 288.13s/it]  6%|▌         | 58/1000 [4:27:49<71:18:22, 272.51s/it]  6%|▌         | 59/1000 [4:32:48<73:18:22, 280.45s/it]  6%|▌         | 60/1000 [4:37:21<72:38:18, 278.19s/it]  6%|▌         | 61/1000 [4:42:20<74:11:43, 284.46s/it]  6%|▌         | 62/1000 [4:47:33<76:21:12, 293.04s/it]  6%|▋         | 63/1000 [4:52:18<75:39:00, 290.65s/it]  6%|▋         | 64/1000 [4:57:07<75:25:05, 290.07s/it]slurmstepd: error: *** STEP 276693.0 ON lrz-dgx-a100-005 CANCELLED AT 2024-02-05T23:55:49 DUE TO TIME LIMIT ***
