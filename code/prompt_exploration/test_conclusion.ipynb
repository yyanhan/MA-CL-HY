{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "import code\n",
    "import code_ablation\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question:str) -> str:\n",
    "    sequences = pipeline(\n",
    "        question,\n",
    "        do_sample=False,\n",
    "        # top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=400,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        return seq['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_conclusion_parse_facts(facts:str)->list:\n",
    "    facts_list = facts.split(',')\n",
    "    facts_list = [fact.strip()+'.' for fact in facts_list]\n",
    "    return facts_list\n",
    "\n",
    "def ablation_conclusion_formulate_facts_number(facts:list)->str:\n",
    "    res = \"\"\n",
    "    for index, fact in enumerate(facts):\n",
    "        res += f\"{index+1}. {fact}\\n\"\n",
    "    return res\n",
    "\n",
    "def ablation_conclusion_formulate_factstr_factnum(facts:str)->str:\n",
    "    fact_list = ablation_conclusion_parse_facts(facts)\n",
    "    fact_num_list = ablation_conclusion_formulate_facts_number(fact_list)\n",
    "    return fact_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_conclusion_result_extractor(prompt, result):\n",
    "    result = result.strip()\n",
    "    if prompt in result:\n",
    "        result = result.replace(prompt, \"\")\n",
    "    result = result.strip()\n",
    "    if '</Answer>' in result:\n",
    "        result = result.split('</Answer>')[0].strip()\n",
    "    result = result.strip()\n",
    "    if 'yes' in result.lower() and 'no' not in result.lower():\n",
    "        return 'yes', result\n",
    "    elif 'yes' not in result.lower() and 'no' in result.lower():\n",
    "        return 'no', result\n",
    "    else:\n",
    "        return 'error', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def evaluate_dev(prompt_answer_list:dict):\n",
    "\n",
    "    acc = 0\n",
    "    n_err = 0\n",
    "    n = 0\n",
    "    res_dict_num = defaultdict(int)\n",
    "    gold_answer = ''\n",
    "    for prompt_answer in prompt_answer_list:\n",
    "        prompt = prompt_answer['prompt']\n",
    "        gold_answer = prompt_answer['answer']\n",
    "        pos = prompt_answer['pos']\n",
    "        num_fact = prompt_answer['num_fact']\n",
    "\n",
    "        correct = False\n",
    "        error   = False\n",
    "\n",
    "        answer  = ask(prompt)\n",
    "        answer_label, answer_full = ablation_conclusion_result_extractor(prompt, answer)\n",
    "        if answer_label == 'error':\n",
    "            error = True\n",
    "        elif answer_label == gold_answer:\n",
    "            correct = True\n",
    "        # print(f\"[{n}] num[{num_fact}] {'[correct]' if correct else '[error]' if error else '[wrong]'}\\tGold:[{gold_answer}]; Ans:{answer_label}; {answer_full}\")\n",
    "        # compare with True Answer\n",
    "        n += 1\n",
    "        if error:\n",
    "                n_err += 1\n",
    "        elif correct:\n",
    "            acc += 1\n",
    "            res_dict_num[num_fact] += 1\n",
    "\n",
    "    print(f\"\"\"Pos:{pos}, gold_answer:{gold_answer}\n",
    "Acc:\\t{acc}%\n",
    "Err:\\t{n_err}\"\"\")\n",
    "    for key, value in res_dict_num.items():\n",
    "        print(f'{key}:\\t{value}/10')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dev_comparison(prompt_answer_list:dict):\n",
    "\n",
    "    acc = 0\n",
    "    n_err = 0\n",
    "    n = 0\n",
    "    gold_answer = ''\n",
    "    for prompt_answer in prompt_answer_list:\n",
    "        prompt = prompt_answer['prompt']\n",
    "        gold_answer = prompt_answer['answer']\n",
    "\n",
    "        correct = False\n",
    "        error   = False\n",
    "\n",
    "        answer  = ask(prompt)\n",
    "        answer_label, answer_full = ablation_conclusion_result_extractor(prompt, answer)\n",
    "        if answer_label == 'error':\n",
    "            error = True\n",
    "        elif answer_label == gold_answer:\n",
    "            correct = True\n",
    "        print(f\"[{n}] {'[correct]' if correct else '[error]' if error else '[wrong]'}\\tGold:[{gold_answer}]; Ans:{[answer_label]}; {answer_full}\")\n",
    "        # compare with True Answer\n",
    "        n += 1\n",
    "        if error:\n",
    "                n_err += 1\n",
    "        elif correct:\n",
    "            acc += 1\n",
    "\n",
    "    print(f\"\"\"gold_answer:{gold_answer}\n",
    "Acc:\\t{acc}/10\n",
    "Err:\\t{n_err}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dev_comparison_same_diff(prompt_answer_list:dict):\n",
    "\n",
    "    acc = 0\n",
    "    acc_T = 0\n",
    "    acc_F = 0\n",
    "    n_err = 0\n",
    "    n = 0\n",
    "    gold_answer = ''\n",
    "    for prompt_answer in prompt_answer_list:\n",
    "        prompt = prompt_answer['prompt']\n",
    "        gold_answer = prompt_answer['answer']\n",
    "\n",
    "        correct = False\n",
    "        error   = False\n",
    "\n",
    "        answer  = ask(prompt)\n",
    "        answer_label, answer_full = ablation_conclusion_result_extractor(prompt, answer)\n",
    "        if answer_label == 'error':\n",
    "            error = True\n",
    "        elif answer_label == gold_answer:\n",
    "            correct = True\n",
    "        print(f\"[{n}] {'[correct]' if correct else '[error]' if error else '[wrong]'}\\tGold:[{gold_answer}]; Ans:{[answer_label]}; {answer_full}\")\n",
    "        # compare with True Answer\n",
    "        n += 1\n",
    "        if error:\n",
    "                n_err += 1\n",
    "        elif correct:\n",
    "            acc += 1\n",
    "            if gold_answer == 'yes':\n",
    "                 acc_T += 1\n",
    "            elif gold_answer == 'no':\n",
    "                 acc_F += 1\n",
    "\n",
    "    print(f\"\"\"gold_answer:{gold_answer}\n",
    "Acc:\\t{acc}/100\n",
    "AccT:\\t{acc_T}/50\n",
    "AccF:\\t{acc_F}/50\n",
    "Err:\\t{n_err}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the query can be found in the list of fact:\n",
    "Fact: Tom is good\n",
    "Query: Tom is good\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the query can be found in the list of fact:\n",
      "Fact: Tom is good\n",
      "Query: Tom is good\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: Sarah is tall\n",
      "Query: Sarah is tall\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: John is short\n",
      "Query: John is short\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The sky is blue\n",
      "Query: The sky is blue\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: Water is wet\n",
      "Query: Water is wet\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The earth is round\n",
      "Query: The earth is round\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The sun is hot\n",
      "Query: The sun is hot\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The moon is cold\n",
      "Query: The moon is cold\n",
      "Answer\n",
      "No\n",
      "\n",
      "Fact: The earth orbits the sun\n",
      "Query: The earth orbits the sun\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The sun is the center of the solar system\n",
      "Query: The sun is the center of the solar system\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The universe is vast\n",
      "Query: The universe is vast\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The universe is expanding\n",
      "Query: The universe is expanding\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The Big Bang theory is the leading explanation for the origin of the universe\n",
      "Query: The Big Bang theory is the leading explanation for the origin of the universe\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The universe is estimated to be around 13.8 billion years old\n",
      "Query: The universe is estimated to be around 13.8 billion years old\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The Milky Way galaxy is home to hundreds of billions of stars\n",
      "Query: The Milky Way galaxy is home to hundreds of billions of stars\n",
      "Answer\n",
      "Yes\n",
      "\n",
      "Fact: The nearest star to the Earth is Proxima Centauri\n",
      "Query: The nearest star to the Earth is Proxima\n"
     ]
    }
   ],
   "source": [
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the query can be found in the list of fact:\n",
      "<Fact> \n",
      "1. Tom is not good\n",
      "<Query> Tom is good\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the query can be found in the list of fact:\n",
    "<Fact> \n",
    "1. Tom is not good\n",
    "<Query> Tom is good\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the query can be found in the list of fact:\n",
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly\n",
      "<Query> Tom is good\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Please let me know if you need any further clarification.\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the query can be found in the list of fact:\n",
    "<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly\n",
    "<Query> Tom is good\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the query in the list of fact:\n",
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly\n",
      "<Query> Tom is good\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "Answer\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the query in the list of fact:\n",
    "<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly\n",
    "<Query> Tom is good\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly\n",
      "</Fact>\n",
      "<Query> Tom is good </Query>\n",
      "please answer if the query mentioned above,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly\n",
    "</Fact>\n",
    "<Query> Tom is good </Query>\n",
    "please answer if the query mentioned above,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly,\n",
      "3. Bob is good,\n",
      "4. Alice is pretty,\n",
      "5. Tom is cute\n",
      "</Fact>\n",
      "<Query> Tom is good </Query>\n",
      "please answer if the query mentioned above,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Yes, the query \"Tom is good\" is true based on the facts provided.\n",
      "\n",
      "Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly,\n",
    "3. Bob is good,\n",
    "4. Alice is pretty,\n",
    "5. Tom is cute\n",
    "</Fact>\n",
    "<Query> Tom is good </Query>\n",
    "please answer if the query mentioned above,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly,\n",
      "3. Bob is good,\n",
      "4. Alice is pretty,\n",
      "5. Tom is cute\n",
      "</Fact>\n",
      "<Query> Tom is not good </Query>\n",
      "please answer if the query mentioned above,\n",
      "please check the spellation strictly,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly,\n",
    "3. Bob is good,\n",
    "4. Alice is pretty,\n",
    "5. Tom is cute\n",
    "</Fact>\n",
    "<Query> Tom is not good </Query>\n",
    "please answer if the query mentioned above,\n",
    "please check the spellation strictly,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly,\n",
      "3. Bob is good,\n",
      "4. Alice is pretty,\n",
      "5. Tom is cute\n",
      "</Fact>\n",
      "please answer if the query \"Tom is not good\" mentioned above,\n",
      "please check the spellation strictly,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly,\n",
    "3. Bob is good,\n",
    "4. Alice is pretty,\n",
    "5. Tom is cute\n",
    "</Fact>\n",
    "please answer if the query \"Tom is not good\" mentioned above,\n",
    "please check the spellation strictly,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fact> \n",
      "1. Tom is not good,\n",
      "2. Tom is ugly,\n",
      "3. Bob is good,\n",
      "4. Alice is pretty,\n",
      "5. Tom is cute\n",
      "</Fact>\n",
      "please answer if the <Query>\"Tom is not good\"</Query> mentioned above,\n",
      "please check the spellation strictly,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Fact> \n",
    "1. Tom is not good,\n",
    "2. Tom is ugly,\n",
    "3. Bob is good,\n",
    "4. Alice is pretty,\n",
    "5. Tom is cute\n",
    "</Fact>\n",
    "please answer if the <Query>\"Tom is not good\"</Query> mentioned above,\n",
    "please check the spellation strictly,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Fact> \n",
      "Tom is not good,\n",
      "Tom is ugly,\n",
      "Bob is good,\n",
      "Alice is pretty,\n",
      "Tom is cute\n",
      "</Fact>\n",
      "please answer if the <Query>\"Tom is not good\"</Query> mentioned above,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Fact> \n",
    "Tom is not good,\n",
    "Tom is ugly,\n",
    "Bob is good,\n",
    "Alice is pretty,\n",
    "Tom is cute\n",
    "</Fact>\n",
    "please answer if the <Query>\"Tom is not good\"</Query> mentioned above,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Context>: \n",
      "Tom is not good,\n",
      "Tom is ugly,\n",
      "Bob is good,\n",
      "Alice is pretty,\n",
      "Tom is cute\n",
      "</Context>\n",
      "please answer if the sentence \"Tom is cute\" mentioned above,\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"<Context>: \n",
    "Tom is not good,\n",
    "Tom is ugly,\n",
    "Bob is good,\n",
    "Alice is pretty,\n",
    "Tom is cute\n",
    "</Context>\n",
    "please answer if the sentence \"Tom is cute\" mentioned above,\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the <Query> is mentioned in <Context>,\n",
      "<Query>Tom is king</Query>\n",
      "<Context>: \n",
      "Tom is not good,\n",
      "Tom is ugly,\n",
      "Bob is good,\n",
      "Alice is pretty,\n",
      "Tom is cute,\n",
      "</Context>\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the <Query> is mentioned in <Context>,\n",
    "<Query>Tom is king</Query>\n",
    "<Context>: \n",
    "Tom is not good,\n",
    "Tom is ugly,\n",
    "Bob is good,\n",
    "Alice is pretty,\n",
    "Tom is cute,\n",
    "</Context>\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the <Query> is mentioned in story,\n",
      "<Query>Tom is king</Query>\n",
      "<story>: \n",
      "Tom is not good, Tom is ugly, Bob is good, Alice is pretty, Tom is cute\n",
      "</story>\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the <Query> is mentioned in story,\n",
    "<Query>Tom is king</Query>\n",
    "<story>: \n",
    "Tom is not good, Tom is ugly, Bob is good, Alice is pretty, Tom is cute\n",
    "</story>\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please answer if the <Query> is mentioned in text,\n",
      "<Query>Tom is king</Query>\n",
      "<text>: \n",
      "Tom is not good, Tom is ugly, Bob is good, Alice is pretty, Tom is cute\n",
      "</text>\n",
      "please answer with 'yes' or 'no',\n",
      "please answer with the format:\n",
      "Answer: <your answer here>\n",
      "Answer\n",
      "\n",
      "Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(facts:str)->str:\n",
    "    prompt = \"\"\"please answer if the <Query> is mentioned in text,\n",
    "<Query>Tom is king</Query>\n",
    "<text>: \n",
    "Tom is not good, Tom is ugly, Bob is good, Alice is pretty, Tom is cute\n",
    "</text>\n",
    "please answer with 'yes' or 'no',\n",
    "please answer with the format:\n",
    "Answer: <your answer here>\n",
    "Answer\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(\"\")\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \"\n",
      "sentence 1. Tom is not good, \n",
      "sentence 2. Tom is ugly, \n",
      "sentence 3. Bob is good, \n",
      "sentence 4. Alice is pretty, \n",
      "sentence 5. Tom is cute\"\n",
      "\"\n",
      "Query: \"Bob is not good\"\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
      "If yes, please tell me which sentence is it.\n",
      "Please pay attention to spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context>\n",
      "Output:\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Please let me know if you have any questions or concerns.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bob is not good\"\n",
    "context = \"\"\"\n",
    "sentence 1. Tom is not good, \n",
    "sentence 2. Tom is ugly, \n",
    "sentence 3. Bob is good, \n",
    "sentence 4. Alice is pretty, \n",
    "sentence 5. Tom is cute\"\n",
    "\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Context: \"{context}\"\n",
    "Query: \"{query}\"\n",
    "Task: Determine if the query is mentioned in the given context.\n",
    "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "If yes, please tell me which sentence is it.\n",
    "Please pay attention to spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \"\n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom)\"\n",
      "\"\n",
      "Query: \"Bob is not good\"\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
      "If yes, please tell me which sentence is it.\n",
      "Please pay attention to spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context>\n",
      "Output:\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Please let me know if you have any questions or concerns.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bob is not good\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\n",
    "\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Context: \"{context}\"\n",
    "Query: \"{query}\"\n",
    "Task: Determine if the query is mentioned in the given context.\n",
    "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "If yes, please tell me which sentence is it.\n",
    "Please pay attention to spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom)\"\n",
      "\n",
      "Query: \"Not Good(Bob)\"\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
      "If yes, please tell me which sentence is it.\n",
      "Please pay attention to spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context>\n",
      "Output:\n",
      "\n",
      "Answer: Yes\n",
      "Location: Sentence 1\n",
      "\n",
      "Please let me know if you have any questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "query = \"Not Good(Bob)\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\n",
    "\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "Query: \"{query}\"\n",
    "Task: Determine if the query is mentioned in the given context.\n",
    "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "If yes, please tell me which sentence is it.\n",
    "Please pay attention to spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom)\n",
      "Query: Bad(Bob)\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
      "If yes, please tell me which sentence is it.\n",
      "Please pay attention to spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context>\n",
      "Output:\n",
      "Answer: Yes\n",
      "Location: 1, 2, 3, 4, 5\n",
      "\n",
      "I hope you can help me with this task.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bad(Bob)\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "Query: {query}\n",
    "Task: Determine if the query is mentioned in the given context.\n",
    "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "If yes, please tell me which sentence is it.\n",
    "Please pay attention to spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "Context: \n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom),\n",
      "Query: Bad(Bob)\n",
      "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
      "If yes, please tell me which sentence is it.\n",
      "Please pay attention to spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context>\n",
      "Output:\n",
      "\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Please let me know if you have any questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bad(Bob)\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "Context: {context},\n",
    "Query: {query}\n",
    "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "If yes, please tell me which sentence is it.\n",
    "Please pay attention to spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "Query: Bad(Bob)\n",
      "Context: \n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom),\n",
      "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
      "If yes, please tell me which sentence is it.\n",
      "Please pay attention to spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context>\n",
      "Output:\n",
      "\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Please let me know if you have any questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bad(Bob)\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "Query: {query}\n",
    "Context: {context},\n",
    "Output: if you can find the query, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "If yes, please tell me which sentence is it.\n",
    "Please pay attention to spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given Text.\n",
      "<Query>Bad(Bob)</Query>,\n",
      "<Text>\n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom)</Text>,\n",
      "If you can find the query, please answer \"yes\", else, if you can't find in Text please answer \"no\".\n",
      "Please consider the spelling strictly,\n",
      "If the answer is \"yes\", please tell me which sentence is it, otherwise say NOTHING,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context or with NOTHING>\n",
      "Output:\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Expected Output:\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Actual Output:\n",
      "Answer: No\n",
      "Location: NOTHING\n",
      "\n",
      "Explanation:\n",
      "The query \"Bad(Bob)\" is not mentioned in the given text. Therefore, the answer is \"No\" and the location is \"NOTHING\".\n"
     ]
    }
   ],
   "source": [
    "query = \"Bad(Bob)\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given Text.\n",
    "<Query>{query}</Query>,\n",
    "<Text>{context}</Text>,\n",
    "If you can find the query, please answer \"yes\", else, if you can't find in Text please answer \"no\".\n",
    "Please consider the spelling strictly,\n",
    "If the answer is \"yes\", please tell me which sentence is it, otherwise say NOTHING,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context or with NOTHING>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given Text.\n",
      "<Query>Bad(Bob)</Query>,\n",
      "<Text>\n",
      "sentence 1. Not Good(Tom), \n",
      "sentence 2. Ugly(Tom), \n",
      "sentence 3. Good(Bob), \n",
      "sentence 4. Pretty(Alice), \n",
      "sentence 5. Cute(Tom)\n",
      "</Text>,\n",
      "If you can find the query, please answer \"yes\", else, if you can't find in Text please answer \"no\".\n",
      "Please consider the spelling strictly,\n",
      "If the answer is \"yes\", please tell me which sentence is it, otherwise say NOTHING,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Location: <your answer here with number of context or with NOTHING>\n",
      "Output:\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Expected Output:\n",
      "Answer: Yes\n",
      "Location: Sentence 3\n",
      "\n",
      "Note: The query \"Bad(Bob)\" is mentioned in sentence 3 of the given text.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bad(Bob)\"\n",
    "context = \"\"\"\n",
    "sentence 1. Not Good(Tom), \n",
    "sentence 2. Ugly(Tom), \n",
    "sentence 3. Good(Bob), \n",
    "sentence 4. Pretty(Alice), \n",
    "sentence 5. Cute(Tom)\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given Text.\n",
    "<Query>{query}</Query>,\n",
    "<Text>{context}\n",
    "</Text>,\n",
    "If you can find the query, please answer \"yes\", else, if you can't find in Text please answer \"no\".\n",
    "Please consider the spelling strictly,\n",
    "If the answer is \"yes\", please tell me which sentence is it, otherwise say NOTHING,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Location: <your answer here with number of context or with NOTHING>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a sentence:\n",
      "<Text>Good(Bob)</Text>,\n",
      "<Query>Bad(Bob)</Query>,\n",
      "Task: does the text talk about the Query.\n",
      "Please answer with yes or no,\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The text talks about the Query \"Bad(Bob)\" because the sentence contains the word \"Bad\" which is the same as the Query. Therefore, the answer is yes.\n"
     ]
    }
   ],
   "source": [
    "query = \"Bad(Bob)\"\n",
    "context = \"\"\"Good(Bob)\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given a sentence:\n",
    "<Text>{context}</Text>,\n",
    "<Query>{query}</Query>,\n",
    "Task: does the text talk about the Query.\n",
    "Please answer with yes or no,\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a sentence:\n",
      "<Text>z4y9q</Text>,\n",
      "<Query>3h1Yz</Query>,\n",
      "Task: does the text talk about the Query.\n",
      "Please answer with yes or no,\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The text contains the word \"3h1Yz\" which is the same as the Query. Therefore, the text talks about the Query.\n"
     ]
    }
   ],
   "source": [
    "query = \"3h1Yz\"\n",
    "context = \"\"\"z4y9q\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given a sentence:\n",
    "<Text>{context}</Text>,\n",
    "<Query>{query}</Query>,\n",
    "Task: does the text talk about the Query.\n",
    "Please answer with yes or no,\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a sentence:\n",
      "<Sentence>z4y9q</Sentence>,\n",
      "<Query>3h1Yz</Query>,\n",
      "Task: does the text talk about the Query.\n",
      "Please answer with yes or no,\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The sentence \"z4y9q\" does not contain the word \"3h1Yz\", therefore the text does not talk about the Query.\n",
      "\n",
      "Please let me know if you have any questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "query = \"3h1Yz\"\n",
    "context = \"\"\"z4y9q\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given a sentence:\n",
    "<Sentence>{context}</Sentence>,\n",
    "<Query>{query}</Query>,\n",
    "Task: does the text talk about the Query.\n",
    "Please answer with yes or no,\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a sentence:\n",
      "<Sentence>z4y9q</Sentence>,\n",
      "<Query>3h1Yz</Query>,\n",
      "Task: Is the sentence same as the Query.\n",
      "Please answer with yes or no,\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The query is \"3h1Yz\" and the sentence is \"z4y9q\". These two strings are the same when considering only the spelling, so the answer is yes.\n"
     ]
    }
   ],
   "source": [
    "query = \"3h1Yz\"\n",
    "context = \"\"\"z4y9q\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given a sentence:\n",
    "<Sentence>{context}</Sentence>,\n",
    "<Query>{query}</Query>,\n",
    "Task: Is the sentence same as the Query.\n",
    "Please answer with yes or no,\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two sentences:\n",
      "<Sentence 1>z4y9q</Sentence 1>,\n",
      "<Sentence 2>3h1Yz</Sentence 2>,\n",
      "Task: Are the  two sentences the same?\n",
      "Please answer with yes or no,\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The two sentences are the same, as they have the same words and the same order of words.\n",
      "\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "query = \"3h1Yz\"\n",
    "context = \"\"\"z4y9q\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given two sentences:\n",
    "<Sentence 1>{context}</Sentence 1>,\n",
    "<Sentence 2>{query}</Sentence 2>,\n",
    "Task: Are the  two sentences the same?\n",
    "Please answer with yes or no,\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two sentences:\n",
      "<Sentence 1>z4y9q</Sentence 1>,\n",
      "<Sentence 2>3h1Yz</Sentence 2>,\n",
      "Task: Are the  two sentences the same?\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The two sentences are the same, as they have the same words and the same order.\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Explanation:\n",
      "The two sentences are different, as they have different words or different orders of words.\n"
     ]
    }
   ],
   "source": [
    "query = \"3h1Yz\"\n",
    "context = \"\"\"z4y9q\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given two sentences:\n",
    "<Sentence 1>{context}</Sentence 1>,\n",
    "<Sentence 2>{query}</Sentence 2>,\n",
    "Task: Are the  two sentences the same?\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two sentences:\n",
      "<Sentences>\n",
      "Sentence 1: z4y9q\n",
      "Sentence 2: 3h1Yz\n",
      "</Sentences>\n",
      "Task: Are the two sentences the same?\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The two sentences are the same, as they have the same meaning and spelling.\n",
      "\n",
      "Answer: No\n",
      "\n",
      "Explanation:\n",
      "The two sentences are different, as they have different meanings or spelling.\n"
     ]
    }
   ],
   "source": [
    "query = \"3h1Yz\"\n",
    "context = \"\"\"z4y9q\"\"\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given two sentences:\n",
    "<Sentences>\n",
    "Sentence 1: {context}\n",
    "Sentence 2: {query}\n",
    "</Sentences>\n",
    "Task: Are the two sentences the same?\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given two sentences:\n",
      "Sentence 1: <Sentence>Bob is good</Sentence>\n",
      "Sentence 2: <Sentence>Tom is good</Sentence>\n",
      "Task: Are the two sentences the same?\n",
      "Please consider the spelling strictly,\n",
      "Please answer with the format:\n",
      "Answer: <your answer here with yes or no>\n",
      "Output:\n",
      "Answer: Yes\n",
      "\n",
      "Explanation:\n",
      "The two sentences are the same because they both convey the same meaning. The name \"Bob\" and \"Tom\" are interchangeable in this context, and the sentence \"Bob is good\" can be replaced with \"Tom is good\" without changing the meaning of the sentence. Therefore, the two sentences are the same.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Bob is good\"\"\"\n",
    "query = \"Tom is good\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Given two sentences:\n",
    "Sentence 1: <Sentence>{context}</Sentence>\n",
    "Sentence 2: <Sentence>{query}</Sentence>\n",
    "Task: Are the two sentences the same?\n",
    "Please consider the spelling strictly,\n",
    "Please answer with the format:\n",
    "Answer: <your answer here with yes or no>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Are the two sentences the same? Same sentence means thay are strictly the same with spelling and name.\n",
      "<Example>\n",
      "Sentence 1: Alice is bad\n",
      "Sentence 2: Alice is nice\n",
      "Answer: no\n",
      "</Example>\n",
      "<Example>\n",
      "Sentence 1: Alice is bad\n",
      "Sentence 2: Alice is bad\n",
      "Answer: yes\n",
      "</Example>\n",
      "Given two sentences:\n",
      "Sentence 1: <Sentence>Bob is good</Sentence>\n",
      "Sentence 2: <Sentence>Tom is good</Sentence>\n",
      "\n",
      "Please answer with the format:\n",
      "Answer: <your answer>\n",
      "Output:\n",
      "Answer: no\n",
      "\n",
      "Explanation: The two sentences are not the same because they have different subjects. The first sentence mentions Bob, while the second sentence mentions Tom.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Bob is good\"\"\"\n",
    "query = \"Tom is good\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"\n",
    "Task: Are the two sentences the same? Same sentence means thay are strictly the same with spelling and name.\n",
    "<Example>\n",
    "Sentence 1: Alice is bad\n",
    "Sentence 2: Alice is nice\n",
    "Answer: no\n",
    "</Example>\n",
    "<Example>\n",
    "Sentence 1: Alice is bad\n",
    "Sentence 2: Alice is bad\n",
    "Answer: yes\n",
    "</Example>\n",
    "Given two sentences:\n",
    "Sentence 1: <Sentence>{context}</Sentence>\n",
    "Sentence 2: <Sentence>{query}</Sentence>\n",
    "\n",
    "Please answer with the format:\n",
    "Answer: <your answer>\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Are the two sentences the same? \n",
      "<Example 1>\n",
      "Sentence 1: \"Alice is bad\",\n",
      "Sentence 2: \"Alice is nice\",\n",
      "Answer: no\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "Sentence 1: \"Alice is bad\",\n",
      "Sentence 2: \"Alice is bad\",\n",
      "Answer: yes\n",
      "</Example 2>\n",
      "Given two sentences:\n",
      "<Sentence>\n",
      "Sentence 1: \"Bob is good\"\n",
      "Sentence 2: \"Bob is good\"\n",
      "</Sentence>\n",
      "If the two sentences are the same, please answer \"yes\", else, the two sentences are not the same please answer \"no\".\n",
      "Please answer with the format:\n",
      "Answer: <your answer>\n",
      "Output:\n",
      "Answer: no\n",
      "\n",
      "Expected output:\n",
      "Answer: no\n",
      "\n",
      "Note: The two sentences are not the same because they have the same words but in a different order.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Bob is good\"\"\"\n",
    "query = \"Bob is good\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"\n",
    "Task: Are the two sentences the same? \n",
    "<Example 1>\n",
    "Sentence 1: \"Alice is bad\",\n",
    "Sentence 2: \"Alice is nice\",\n",
    "Answer: no\n",
    "</Example 1>\n",
    "<Example 2>\n",
    "Sentence 1: \"Alice is bad\",\n",
    "Sentence 2: \"Alice is bad\",\n",
    "Answer: yes\n",
    "</Example 2>\n",
    "Given two sentences:\n",
    "<Sentence>\n",
    "Sentence 1: \"{context}\"\n",
    "Sentence 2: \"{query}\"\n",
    "</Sentence>\n",
    "If the two sentences are the same, please answer \"yes\", else, the two sentences are not the same please answer \"no\".\n",
    "Please answer with the format:\n",
    "Answer: <your answer>\n",
    "Output:\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Are the two sentences the same? \n",
      "<Example 1>\n",
      "Sentence 1: \"Good(Alice)\",\n",
      "Sentence 2: \"Bad(Alice)\",\n",
      "Answer: no\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "Sentence 1: \"Good(Bob)\",\n",
      "Sentence 2: \"Good(Bob)\",\n",
      "Answer: yes\n",
      "</Example 2>\n",
      "Given two sentences:\n",
      "<Sentence>\n",
      "Sentence 1: \"Good(Bob)\"\n",
      "Sentence 2: \"Good(Bob)\"\n",
      "</Sentence>\n",
      "If the two sentences are the same, please answer \"yes\", else, the two sentences are not the same please answer \"no\".\n",
      "Please answer with the format:\n",
      "Answer: <your answer>\n",
      "Output:\n",
      "Answer: no\n",
      "\n",
      "Expected output:\n",
      "Answer: no\n",
      "\n",
      "Note: The task is to check if the two sentences are the same or not based on the word \"Good\" and the name \"Bob\".\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Good(Bob)\"\"\"\n",
    "query = \"Good(Bob)\"\n",
    "\n",
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"\n",
    "Task: Are the two sentences the same? \n",
    "<Example 1>\n",
    "Sentence 1: \"Good(Alice)\",\n",
    "Sentence 2: \"Bad(Alice)\",\n",
    "Answer: no\n",
    "</Example 1>\n",
    "<Example 2>\n",
    "Sentence 1: \"Good(Bob)\",\n",
    "Sentence 2: \"Good(Bob)\",\n",
    "Answer: yes\n",
    "</Example 2>\n",
    "Given two sentences:\n",
    "<Sentence>\n",
    "Sentence 1: \"{context}\"\n",
    "Sentence 2: \"{query}\"\n",
    "</Sentence>\n",
    "If the two sentences are the same, please answer \"yes\", else, the two sentences are not the same please answer \"no\".\n",
    "Please answer with the format:\n",
    "Answer: <your answer>\n",
    "Output:\"\"\"\n",
    "    return prompt\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion with Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. Bob is nice. \n",
      "2. Bob is good. \n",
      "3. Bob is funny.\n",
      "Query: Bob is nice.\n",
      "Answer: yes, because the query can be found in the context 1.\n",
      "Context: \n",
      "1. Bob is good. \n",
      "2. Bob is cute.\n",
      "Query: Bob is nice.\n",
      "Answer: no, because the query can not be found in the context.\n",
      "</Example>\n",
      "<Context>\n",
      "Context: Tom is good.\n",
      "</Context>\n",
      "<Query>\n",
      "Query: Tom is good.\n",
      "</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Output>\n",
      "Answer: yes\n",
      "</Output>\n",
      "</Task>\n",
      " yes\n",
      "</Output>\n",
      "</Task>\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If you can find the query in the context, please answer \"yes\", else, if you can't find in context please answer \"no\".\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. Bob is nice. \n",
    "2. Bob is good. \n",
    "3. Bob is funny.\n",
    "Query: Bob is nice.\n",
    "Answer: yes, because the query can be found in the context 1.\n",
    "Context: \n",
    "1. Bob is good. \n",
    "2. Bob is cute.\n",
    "Query: Bob is nice.\n",
    "Answer: no, because the query can not be found in the context.\n",
    "</Example>\n",
    "<Context>\n",
    "Context: {context}\n",
    "</Context>\n",
    "<Query>\n",
    "Query: {query}\n",
    "</Query>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Output>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"Tom is good.\"\n",
    "query = \"Tom is good.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "print(answer.replace(prompt, ''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbering + Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. Bob is nice. \n",
      "2. Bob is good. \n",
      "3. Bob is funny.\n",
      "Query: Bob is nice.\n",
      "Answer: yes, because the query can be found in the context 1.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. Bob is good. \n",
      "2. Bob is cute.\n",
      "3. Bob is funny.\n",
      "Query: Bob is cute.\n",
      "Answer: yes, because the query can be found in the context 2.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. Bob is good. \n",
      "2. Bob is cute.\n",
      "3. Bob is funny.\n",
      "Query: Bob is nice.\n",
      "Answer: no, because the query can not be found in the context.\n",
      "</Example>\n",
      "<Context>\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "</Context>\n",
      "<Query>\n",
      "Tom is cute.\n",
      "</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer: yes, because the query can be found in the context 1.\n",
      "</Answer>\n",
      "</Question>\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t97%\n",
      "Err:\t0\n",
      "1:\t8/10\n",
      "2:\t10/10\n",
      "3:\t10/10\n",
      "4:\t10/10\n",
      "5:\t10/10\n",
      "6:\t10/10\n",
      "7:\t10/10\n",
      "8:\t10/10\n",
      "9:\t10/10\n",
      "10:\t9/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t5%\n",
      "Err:\t0\n",
      "0:\t1/10\n",
      "4:\t1/10\n",
      "6:\t1/10\n",
      "8:\t2/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If yes, please tell me the position of the query.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. Bob is nice. \n",
    "2. Bob is good. \n",
    "3. Bob is funny.\n",
    "Query: Bob is nice.\n",
    "Answer: yes, because the query can be found in the context 1.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. Bob is good. \n",
    "2. Bob is cute.\n",
    "3. Bob is funny.\n",
    "Query: Bob is cute.\n",
    "Answer: yes, because the query can be found in the context 2.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. Bob is good. \n",
    "2. Bob is cute.\n",
    "3. Bob is funny.\n",
    "Query: Bob is nice.\n",
    "Answer: no, because the query can not be found in the context.\n",
    "</Example>\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. Bob is nice. \n",
      "2. Bob is good. \n",
      "3. Bob is funny.\n",
      "Query: Bob is good.\n",
      "Answer: yes, because the query can be found in the context 2.\n",
      "</Example>\n",
      "<Context>\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "</Context>\n",
      "<Query>\n",
      "Tom is cute.\n",
      "</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer: no, because the query is not mentioned in the given context.\n",
      "</Answer>\n",
      "</Exam>\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t28%\n",
      "Err:\t0\n",
      "4:\t1/10\n",
      "5:\t1/10\n",
      "6:\t5/10\n",
      "7:\t3/10\n",
      "8:\t6/10\n",
      "9:\t4/10\n",
      "10:\t8/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t70%\n",
      "Err:\t0\n",
      "0:\t10/10\n",
      "1:\t10/10\n",
      "2:\t9/10\n",
      "3:\t6/10\n",
      "4:\t8/10\n",
      "5:\t5/10\n",
      "6:\t8/10\n",
      "7:\t6/10\n",
      "8:\t5/10\n",
      "9:\t3/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If yes, please tell me the position of the query.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. Bob is nice. \n",
    "2. Bob is good. \n",
    "3. Bob is funny.\n",
    "Query: Bob is good.\n",
    "Answer: yes, because the query can be found in the context 2.\n",
    "</Example>\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Context>\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "</Context>\n",
      "<Query>\n",
      "Tom is cute.\n",
      "</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Answer>\n",
      "</Question>\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t78%\n",
      "Err:\t0\n",
      "1:\t8/10\n",
      "2:\t7/10\n",
      "3:\t5/10\n",
      "4:\t8/10\n",
      "5:\t9/10\n",
      "6:\t7/10\n",
      "7:\t7/10\n",
      "8:\t10/10\n",
      "9:\t8/10\n",
      "10:\t9/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t51%\n",
      "Err:\t0\n",
      "0:\t5/10\n",
      "1:\t9/10\n",
      "2:\t9/10\n",
      "3:\t4/10\n",
      "4:\t3/10\n",
      "5:\t4/10\n",
      "6:\t7/10\n",
      "7:\t3/10\n",
      "8:\t4/10\n",
      "9:\t3/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If yes, please tell me the position of the query.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "</Example>\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "<Example 1>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example 2>\n",
      "</Example>\n",
      "<Context>\n",
      "1. Tom is nice.\n",
      "2. Tom is good.</Context>\n",
      "<Query>Tom is cute.</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Answer>\n",
      "</Solution> \n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t71%\n",
      "Err:\t0\n",
      "2:\t3/10\n",
      "3:\t6/10\n",
      "4:\t9/10\n",
      "5:\t9/10\n",
      "6:\t9/10\n",
      "7:\t7/10\n",
      "8:\t10/10\n",
      "9:\t9/10\n",
      "10:\t9/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t31%\n",
      "Err:\t0\n",
      "0:\t7/10\n",
      "1:\t10/10\n",
      "2:\t5/10\n",
      "3:\t1/10\n",
      "4:\t1/10\n",
      "5:\t1/10\n",
      "6:\t3/10\n",
      "7:\t1/10\n",
      "8:\t1/10\n",
      "9:\t1/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If yes, please tell me the position of the query.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "<Example 1>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "</Example 1>\n",
    "<Example 2>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "</Example 2>\n",
    "</Example>\n",
    "<Context>\n",
    "{context}</Context>\n",
    "<Query>{query}</Query>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Context>\n",
      "1. Tom is nice.\n",
      "2. Tom is good.</Context>\n",
      "<Query>Tom is cute.</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the context.\n",
      "</Answer>\n",
      "</Question>\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Context>\n",
      "1. The lion eats the bald eagle.\n",
      "</Context>\n",
      "<Query>The lion eats the bald eagle.</Query>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer:\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t73%\n",
      "Err:\t0\n",
      "1:\t1/10\n",
      "2:\t2/10\n",
      "3:\t8/10\n",
      "4:\t10/10\n",
      "5:\t9/10\n",
      "6:\t9/10\n",
      "7:\t7/10\n",
      "8:\t10/10\n",
      "9:\t8/10\n",
      "10:\t9/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t38%\n",
      "Err:\t0\n",
      "0:\t7/10\n",
      "1:\t10/10\n",
      "2:\t6/10\n",
      "3:\t1/10\n",
      "4:\t1/10\n",
      "5:\t3/10\n",
      "6:\t5/10\n",
      "7:\t3/10\n",
      "8:\t2/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If yes, please tell me the position of the query.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "</Example>\n",
    "<Context>\n",
    "{context}</Context>\n",
    "<Query>{query}</Query>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Question>\n",
      "Context:\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "Query:Tom is cute.\n",
      "</Question>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Answer>\n",
      "</Question>\n",
      "<Question>\n",
      "Context:\n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: John is smart.\n",
      "</Question>\n",
      "<Answer>\n",
      "Answer: yes, because the query can be found in the contexts 1 and 3.\n",
      "</Answer>\n",
      "</Question>\n",
      "<Question>\n",
      "Context:\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "Query: Tom is tall.\n",
      "</Question>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Answer>\n",
      "</Question>\n",
      "<Question>\n",
      "Context:\n",
      "1. John is kind, \n",
      "2. Fiona is smart,\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "If yes, please tell me the position of the query.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Question>\n",
      "Context:\n",
      "1. The lion eats the bald eagle.\n",
      "Query:The lion eats the bald eagle.\n",
      "</Question>\n",
      "<Format>\n",
      "Please answer with the format:\n",
      "Answer: <your answer here>\n",
      "</Format>\n",
      "<Answer>\n",
      "Answer:\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t53%\n",
      "Err:\t0\n",
      "2:\t1/10\n",
      "3:\t4/10\n",
      "4:\t4/10\n",
      "5:\t7/10\n",
      "6:\t6/10\n",
      "7:\t7/10\n",
      "8:\t9/10\n",
      "9:\t8/10\n",
      "10:\t7/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t66%\n",
      "Err:\t0\n",
      "0:\t10/10\n",
      "1:\t10/10\n",
      "2:\t8/10\n",
      "3:\t7/10\n",
      "4:\t5/10\n",
      "5:\t6/10\n",
      "6:\t7/10\n",
      "7:\t5/10\n",
      "8:\t5/10\n",
      "9:\t3/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "If yes, please tell me the position of the query.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind.\n",
    "2. Fiona is smart. \n",
    "3. Chris is kind.\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind.\n",
    "2. Fiona is smart. \n",
    "3. Chris is kind.\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "</Example>\n",
    "<Question>\n",
    "Context:\n",
    "{context}Query: {query}\n",
    "</Question>\n",
    "<Format>\n",
    "Please answer with the format:\n",
    "Answer: <your answer here>\n",
    "</Format>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\\n\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83-40 (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Question>\n",
      "Context:\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "Query: Tom is cute.\n",
      "</Question>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Answer>\n",
      "\n",
      "Explanation:\n",
      "In this task, you will be given a context and a query. Your task is to determine if the query is mentioned in the given context.\n",
      "\n",
      "For each example, you will see a context and a query. You need to determine if the query is mentioned in the context. If the query is mentioned, you should answer \"yes\". If the query is not mentioned, you should answer \"no\".\n",
      "\n",
      "For the first example, the query \"Bob is good\" is not mentioned in the context, so you should answer \"no\".\n",
      "\n",
      "For the second example, the query \"Fiona is smart\" is mentioned in the context, so you should answer \"yes\".\n",
      "\n",
      "For the third example, the query \"Tom is cute\" is not mentioned in the context, so you should answer \"no\".\n",
      "\n",
      "For the last example, the query \"Tom is nice\" is mentioned in the context, so you should answer\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "Please pay attention to spelling strictly,\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind, \n",
      "2. Fiona is smart, \n",
      "3. Chris is kind\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Question>\n",
      "Context:\n",
      "1. The lion eats the bald eagle.\n",
      "Query: The lion eats the bald eagle.\n",
      "</Question>\n",
      "<Answer>\n",
      "Answer:\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t83%\n",
      "Err:\t0\n",
      "1:\t8/10\n",
      "2:\t8/10\n",
      "3:\t8/10\n",
      "4:\t9/10\n",
      "5:\t9/10\n",
      "6:\t9/10\n",
      "7:\t8/10\n",
      "8:\t9/10\n",
      "9:\t8/10\n",
      "10:\t7/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t40%\n",
      "Err:\t0\n",
      "0:\t7/10\n",
      "1:\t7/10\n",
      "2:\t6/10\n",
      "4:\t1/10\n",
      "5:\t3/10\n",
      "6:\t4/10\n",
      "7:\t4/10\n",
      "8:\t5/10\n",
      "9:\t3/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "Please pay attention to spelling strictly,\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind, \n",
    "2. Fiona is smart, \n",
    "3. Chris is kind\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "</Example>\n",
    "<Question>\n",
    "Context:\n",
    "{context}Query: {query}\n",
    "</Question>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\\n\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81-36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the query is mentioned in the given context.\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart.\n",
      "3. Chris is kind.\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "3. Chris is kind.\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Question>\n",
      "Context:\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "Query: Tom is cute.\n",
      "</Question>\n",
      "<Answer>\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Answer>\n",
      "</Question>\n",
      "</Task>\n",
      "\n",
      "This task is designed to test the ability of the model to understand the context of a query and determine if it is mentioned in the given context. The model should be able to identify the context and determine if the query is present in it.\n",
      "\n",
      "For example, in the first example, the query \"Bob is good\" is not mentioned in the context, so the answer should be \"no\". In the second example, the query \"Fiona is smart\" is mentioned in the context, so the answer should be \"yes\".\n",
      "\n",
      "In the third example, the query \"Tom is cute\" is not mentioned in the context, so the answer should be \"no\".\n",
      "\n",
      "This task can be used to evaluate the ability of the model to understand the context of a query and determine if it is mentioned in the given context, which is an important aspect of natural language processing and information retrieval.\n",
      "Task: Determine if the query is mentioned in the given context.\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart.\n",
      "3. Chris is kind.\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "</Example>\n",
      "<Example>\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "3. Chris is kind.\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "</Example>\n",
      "<Question>\n",
      "Context:\n",
      "1. The lion eats the bald eagle.\n",
      "Query: The lion eats the bald eagle.\n",
      "</Question>\n",
      "<Answer>\n",
      "Answer:\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t81%\n",
      "Err:\t0\n",
      "1:\t8/10\n",
      "2:\t7/10\n",
      "3:\t8/10\n",
      "4:\t9/10\n",
      "5:\t9/10\n",
      "6:\t9/10\n",
      "7:\t7/10\n",
      "8:\t8/10\n",
      "9:\t9/10\n",
      "10:\t7/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t36%\n",
      "Err:\t0\n",
      "0:\t5/10\n",
      "1:\t6/10\n",
      "2:\t4/10\n",
      "4:\t1/10\n",
      "5:\t3/10\n",
      "6:\t5/10\n",
      "7:\t4/10\n",
      "8:\t5/10\n",
      "9:\t3/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the query is mentioned in the given context.\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind.\n",
    "2. Fiona is smart.\n",
    "3. Chris is kind.\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "</Example>\n",
    "<Example>\n",
    "Context: \n",
    "1. John is kind.\n",
    "2. Fiona is smart. \n",
    "3. Chris is kind.\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "</Example>\n",
    "<Question>\n",
    "Context:\n",
    "{context}Query: {query}\n",
    "</Question>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\\n\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''Task''': Determine if the query is mentioned in the given context.\n",
      "'''Example'''\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart.\n",
      "3. Chris is kind.\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "'''Example'''\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "3. Chris is kind.\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "'''Question'''\n",
      "Context:\n",
      "1. Tom is nice.\n",
      "2. Tom is good.\n",
      "Query: Tom is cute.\n",
      "'''Answer'''\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "\n",
      "This is a simple example of how a language model can be trained to answer questions based on the context of a given text. The model is trained on a dataset of text and corresponding answers, and learns to predict the answer to a given question based on the context of the text.\n",
      "\n",
      "In this example, the model is trained to answer questions about the characteristics of people, and the context is provided by a set of statements about the people. The model learns to identify the characteristics of the people mentioned in the context, and to answer questions about them based on that information.\n",
      "\n",
      "This type of language model can be useful in a variety of applications, such as chatbots, virtual assistants, and language translation systems. By understanding the context of a given text, the model can provide more accurate and relevant answers to questions, and can help to improve the overall performance of the system.\n",
      "'''Task''': Determine if the query is mentioned in the given context.\n",
      "'''Example'''\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart.\n",
      "3. Chris is kind.\n",
      "Query: Bob is good.\n",
      "Answer: no, because the query can not be found in the contexts.\n",
      "'''Example'''\n",
      "Context: \n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "3. Chris is kind.\n",
      "Query: Fiona is smart.\n",
      "Answer: yes, because the query can be found in the contexts 2.\n",
      "'''Question'''\n",
      "Context:\n",
      "1. The lion eats the bald eagle.\n",
      "Query: The lion eats the bald eagle.\n",
      "'''Answer'''\n",
      "Answer:\n",
      "Pos:0, gold_answer:yes\n",
      "Acc:\t28%\n",
      "Err:\t66\n",
      "1:\t8/10\n",
      "2:\t6/10\n",
      "3:\t4/10\n",
      "4:\t2/10\n",
      "5:\t4/10\n",
      "6:\t1/10\n",
      "7:\t1/10\n",
      "9:\t1/10\n",
      "10:\t1/10\n",
      "\n",
      "Pos:0, gold_answer:no\n",
      "Acc:\t20%\n",
      "Err:\t56\n",
      "0:\t4/10\n",
      "1:\t3/10\n",
      "2:\t2/10\n",
      "3:\t1/10\n",
      "4:\t1/10\n",
      "5:\t3/10\n",
      "6:\t2/10\n",
      "7:\t3/10\n",
      "9:\t1/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_conclusion(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"'''Task''': Determine if the query is mentioned in the given context.\n",
    "'''Example'''\n",
    "Context: \n",
    "1. John is kind.\n",
    "2. Fiona is smart.\n",
    "3. Chris is kind.\n",
    "Query: Bob is good.\n",
    "Answer: no, because the query can not be found in the contexts.\n",
    "'''Example'''\n",
    "Context: \n",
    "1. John is kind.\n",
    "2. Fiona is smart. \n",
    "3. Chris is kind.\n",
    "Query: Fiona is smart.\n",
    "Answer: yes, because the query can be found in the contexts 2.\n",
    "'''Question'''\n",
    "Context:\n",
    "{context}Query: {query}\n",
    "'''Answer'''\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"1. Tom is nice.\n",
    "2. Tom is good.\\n\"\"\"\n",
    "query = \"Tom is cute.\"\n",
    "\n",
    "prompt = formulate_prompt_conclusion(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "# print(answer.replace(prompt, ''))\n",
    "\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts']), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : data['num_fact'],\n",
    "        'answer':'yes',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_conclusion(\n",
    "            ablation_conclusion_formulate_factstr_factnum(\n",
    "                data['facts'][1:]), \n",
    "                ablation_conclusion_parse_facts(data['facts'])[0]), \n",
    "        'num_fact'  : str(int(data['num_fact'])-1),\n",
    "        'answer':'no',\n",
    "        'pos'   : 0,\n",
    "    } for data in dataset['train']]\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the two sentences are the same. Please pay attention to the spelling strictly.\n",
      "<Example 1>\n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "Answer: no\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "1. John is nice.\n",
      "2. John is nice. \n",
      "Answer: yes\n",
      "</Example 2>\n",
      "<Sentence>\n",
      "1: Bob is nice.\n",
      "2: Bob is a nice\n",
      "</Sentence>\n",
      "<Answer>\n",
      "Answer: no\n",
      "</Answer>\n",
      "\n",
      "Explanation:\n",
      "\n",
      "In the first example, the two sentences are different because they have different words. \"Kind\" and \"smart\" are two different adjectives with different meanings.\n",
      "\n",
      "In the second example, the two sentences are the same because they have the same words and the same meaning.\n",
      "\n",
      "In the third example, the two sentences are different because they have different words and different meanings. \"Nice\" is an adjective in both sentences, but the second sentence also includes the word \"a\" which changes the meaning of the sentence.\n",
      "\n",
      "Therefore, the answer to the task is \"yes\" for the second example and \"no\" for the first and third examples.\n"
     ]
    }
   ],
   "source": [
    "def formulate_prompt_comparison(context:str, query:str)->str:\n",
    "    prompt = f\"\"\"Task: Determine if the two sentences are the same. Please pay attention to the spelling strictly.\n",
    "<Example 1>\n",
    "1. John is kind.\n",
    "2. Fiona is smart. \n",
    "Answer: no\n",
    "</Example 1>\n",
    "<Example 2>\n",
    "1. John is nice.\n",
    "2. John is nice. \n",
    "Answer: yes\n",
    "</Example 2>\n",
    "<Sentence>\n",
    "1: {context}\n",
    "2: {query}\n",
    "</Sentence>\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "context = 'Bob is nice.'\n",
    "query   = 'Bob is a nice'\n",
    "prompt = formulate_prompt_comparison(context, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the two sentences are the same. Please pay attention to the spelling strictly.\n",
      "<Example 1>\n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "Answer: no\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "1. John is nice.\n",
      "2. John is nice. \n",
      "Answer: yes\n",
      "</Example 2>\n",
      "<Sentence>\n",
      "1: Erin is round.\n",
      "2: Erin is round.\n",
      "</Sentence>\n",
      "<Answer>\n",
      "Answer:\n",
      "[0] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[1] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[2] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[3] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[4] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[5] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[6] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[7] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[8] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "[9] [correct]\tGold:[yes]; Ans:yes; yes\n",
      "gold_answer:yes\n",
      "Acc:\t10/10\n",
      "Err:\t0\n",
      "\n",
      "Task: Determine if the two sentences are the same. Please pay attention to the spelling strictly.\n",
      "<Example 1>\n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "Answer: no\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "1. John is nice.\n",
      "2. John is nice. \n",
      "Answer: yes\n",
      "</Example 2>\n",
      "<Sentence>\n",
      "1: Erin is round.\n",
      "2: The cat sees the lion.\n",
      "</Sentence>\n",
      "<Answer>\n",
      "Answer:\n",
      "[0] [correct]\tGold:[no]; Ans:no; no\n",
      "[1] [correct]\tGold:[no]; Ans:no; no\n",
      "[2] [correct]\tGold:[no]; Ans:no; no\n",
      "[3] [correct]\tGold:[no]; Ans:no; no\n",
      "[4] [correct]\tGold:[no]; Ans:no; no\n",
      "[5] [correct]\tGold:[no]; Ans:no; no\n",
      "[6] [correct]\tGold:[no]; Ans:no; no\n",
      "[7] [correct]\tGold:[no]; Ans:no; no\n",
      "[8] [correct]\tGold:[no]; Ans:no; no\n",
      "[9] [correct]\tGold:[no]; Ans:no; no\n",
      "gold_answer:no\n",
      "Acc:\t10/10\n",
      "Err:\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_100.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "dataset_fact2 = dataset.filter(lambda data: data['num_fact']==2)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_comparison(\n",
    "            ablation_conclusion_parse_facts(data['facts'])[0],\n",
    "            ablation_conclusion_parse_facts(data['facts'])[0],\n",
    "        ), \n",
    "        'answer':'yes',\n",
    "    } for data in dataset_fact2['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev_comparison(prompt_answer_dict_list)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_comparison(\n",
    "            ablation_conclusion_parse_facts(data['facts'])[0],\n",
    "            ablation_conclusion_parse_facts(data['facts'])[1],\n",
    "        ), \n",
    "        'answer':'no',\n",
    "    } for data in dataset_fact2['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev_comparison(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Determine if the two sentences are the same. Please pay attention to the spelling strictly.\n",
      "<Example 1>\n",
      "1. John is kind.\n",
      "2. Fiona is smart. \n",
      "Answer: no\n",
      "</Example 1>\n",
      "<Example 2>\n",
      "1. John is nice.\n",
      "2. John is nice. \n",
      "Answer: yes\n",
      "</Example 2>\n",
      "<Sentence>\n",
      "1: The rabbit eats the squirrel\n",
      "2: The rabbit eats the squirrel\n",
      "</Sentence>\n",
      "<Answer>\n",
      "Answer:\n",
      "[0] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[1] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[2] [wrong]\tGold:[yes]; Ans:['no']; no\n",
      "[3] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[4] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[5] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[6] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[7] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[8] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[9] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[10] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[11] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[12] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[13] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[14] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[15] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[16] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[17] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[18] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[19] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[20] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[21] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[22] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[23] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[24] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[25] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[26] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[27] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[28] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[29] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[30] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[31] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[32] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[33] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[34] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[35] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[36] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[37] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[38] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[39] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[40] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[41] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[42] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[43] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[44] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[45] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[46] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[47] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[48] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[49] [correct]\tGold:[yes]; Ans:['yes']; yes\n",
      "[50] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[51] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[52] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[53] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[54] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[55] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[56] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[57] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[58] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[59] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[60] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[61] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[62] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[63] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[64] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[65] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[66] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[67] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[68] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[69] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[70] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[71] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[72] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[73] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[74] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[75] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[76] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[77] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[78] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[79] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[80] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[81] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[82] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[83] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[84] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[85] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[86] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[87] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[88] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[89] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[90] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[91] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[92] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[93] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[94] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[95] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[96] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[97] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[98] [correct]\tGold:[no]; Ans:['no']; no\n",
      "[99] [correct]\tGold:[no]; Ans:['no']; no\n",
      "gold_answer:no\n",
      "Acc:\t99/100\n",
      "AccT:\t49/50\n",
      "AccF:\t50/50\n",
      "Err:\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"../dataset/fact/CWA_facts_same_diff_100.jsonl\"\n",
    "dataset_fact_same_diff = load_dataset('json', data_files=path_dev)\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': formulate_prompt_comparison(\n",
    "            data['fact_1'],\n",
    "            data['fact_2'],\n",
    "        ), \n",
    "        'answer':data['answer'],\n",
    "    } for data in dataset_fact_same_diff['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev_comparison_same_diff(prompt_answer_dict_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
