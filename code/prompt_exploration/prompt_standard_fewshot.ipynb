{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "import code\n",
    "import code_ablation\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question:str) -> str:\n",
    "    sequences = pipeline(\n",
    "        question,\n",
    "        do_sample=False,\n",
    "        # top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=400,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        return seq['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_2_str_num(facts:list) -> str:\n",
    "    res = \"\"\n",
    "    for index, fact in enumerate(facts):\n",
    "        res += f\"{index+1}. {fact}\\n\"\n",
    "    return res\n",
    "def list_2_str(facts:list) -> str:\n",
    "    res = \"\"\n",
    "    for index, fact in enumerate(facts):\n",
    "        res += f\"{fact}\\n\"\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"123\".split('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_extractor_inference(prompt, result):\n",
    "    result = result.strip()\n",
    "    if prompt in result:\n",
    "        result = result.replace(prompt, \"\")\n",
    "    if '</Answer>' in result:\n",
    "        result = result.split('</Answer>')[0].strip()\n",
    "    result_lines = result.split('\\n')\n",
    "    answer_label_line = result_lines[0]\n",
    "    if len(result_lines) > 1:\n",
    "        answer_conclusion_line = result_lines[1]\n",
    "    else:\n",
    "        answer_conclusion_line = ''\n",
    "\n",
    "    if 'yes' in answer_label_line.lower() and 'no' not in answer_label_line.lower():\n",
    "        answer_label = 'yes'\n",
    "    elif 'yes' not in answer_label_line.lower() and 'no' in answer_label_line.lower():\n",
    "        answer_label = 'no'\n",
    "    else:\n",
    "        answer_label = 'error'\n",
    "\n",
    "    answer_conclusion_after_produce = answer_conclusion_line.split('Produce:')\n",
    "    if len(answer_conclusion_after_produce) > 1:\n",
    "        answer_conclusion = answer_conclusion_after_produce[1].strip()\n",
    "    else:\n",
    "        answer_conclusion = answer_conclusion_after_produce[0]\n",
    "    \n",
    "    return answer_label, answer_conclusion, answer_conclusion_line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def evaluate_dev(prompt_answer_dict_list:list):\n",
    "\n",
    "    p_acc   = 0\n",
    "    p_acc_T = 0\n",
    "    p_acc_F = 0\n",
    "    c_acc   = 0\n",
    "    c_acc_T = 0\n",
    "    c_acc_F = 0\n",
    "    b_acc   = 0\n",
    "    b_acc_T = 0\n",
    "    b_acc_F = 0\n",
    "    n_err   = 0\n",
    "    n       = 0\n",
    "    num_NOTHING = 0\n",
    "    num_NOTHING_T = 0\n",
    "    num_NOTHING_F = 0\n",
    "\n",
    "    print(f\"[nr] [pred] [conc]; [gold] [conc] [full]\")\n",
    "    for prompt_answer in prompt_answer_dict_list:\n",
    "            prompt      = prompt_answer['prompt']\n",
    "            gold_answer = prompt_answer['answer']\n",
    "            conclusion = prompt_answer['conclusion']\n",
    "            correct_pred = False\n",
    "            correct_conc = False\n",
    "            error   = False\n",
    "\n",
    "            answer  = ask(prompt)\n",
    "            answer_label, answer_conclusion, answer_full = result_extractor_inference(prompt, answer)\n",
    "            if answer_label == 'error':\n",
    "                error = True\n",
    "            elif answer_label == gold_answer:\n",
    "                correct_pred = True\n",
    "            if answer_conclusion == conclusion:\n",
    "                correct_conc = True\n",
    "\n",
    "            # print(f\"[{n}] [{'correct' if correct_pred else 'error' if error else 'wrong'}] [{'correct' if correct_conc else 'wrong'}] [{gold_answer}/{answer_label}] [{conclusion}/{answer_conclusion}] {answer_full}\")\n",
    "            # compare with True Answer\n",
    "            n += 1\n",
    "            if answer_conclusion == 'NOTHING':\n",
    "                num_NOTHING += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    num_NOTHING_T += 1\n",
    "                elif gold_answer == 'no':\n",
    "                    num_NOTHING_F += 1\n",
    "\n",
    "            if error:\n",
    "                 n_err += 1\n",
    "            if correct_pred:\n",
    "                p_acc += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    p_acc_T += 1\n",
    "                elif gold_answer == 'no':\n",
    "                    p_acc_F += 1\n",
    "            \n",
    "            if correct_conc:\n",
    "                c_acc += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    c_acc_T += 1\n",
    "                elif gold_answer == 'False':\n",
    "                    c_acc_F += 1\n",
    "            \n",
    "            if correct_pred and correct_conc:\n",
    "                b_acc += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    b_acc_T += 1\n",
    "                elif gold_answer == 'no':\n",
    "                    b_acc_F += 1\n",
    "\n",
    "            \n",
    "\n",
    "    print(f\"\"\"Pred_Acc*:\\t{p_acc}/100, \n",
    "Pred_TAcc*:\\t{p_acc_T}/50, {p_acc_T*2}%\n",
    "Pred_FAcc*:\\t{p_acc_F}/50, {p_acc_F*2}%\n",
    "\n",
    "Conc_Acc:\\t{c_acc}/100,\n",
    "Conc_TAcc*:\\t{c_acc_T}/50, {c_acc_T*2}%\n",
    "Conc_FAcc:\\t{c_acc_F}/50, {c_acc_F*2}%\n",
    "\n",
    "Both_Acc:\\t{b_acc}/100,\n",
    "Both_TAcc:\\t{b_acc_T}/50, {b_acc_T*2}%\n",
    "Both_FAcc:\\t{b_acc_F}/50, {b_acc_F*2}%\n",
    "\n",
    "num_NOTHING:\\t{num_NOTHING}/100,\n",
    "num_NOTHING_T:\\t{num_NOTHING_T}/50, {num_NOTHING_T*2}%\n",
    "num_NOTHING_F:\\t{num_NOTHING_F}/50, {num_NOTHING_F*2}%\n",
    "\n",
    "Err:\\t{n_err}/100\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: all of the facts and rules are ture. Based on the provided Facts and Rules, please answer: is the Statement true or false?\n",
      "Example Theory: Dave is big. Dave is blue. Dave is furry. Dave is nice. Dave is rough. Dave is round. Dave is white. If Dave is blue and Dave is not furry then Dave is white. If someone is round and not blue then they are nice.\n",
      "Example Query: Dave is nice.\n",
      "Example Answer: True\n",
      "Facts and Rules: Bob is good. If Bob is good then Bob is nice.\n",
      "Statements: Bob is nice.\n",
      "Your Answer: True\n",
      "\n",
      "Please provide the Statement you would like me to evaluate, and I will let you know if it is true or false based on the provided Facts and Rules.\n"
     ]
    }
   ],
   "source": [
    "def proofwriter_get_prompt(theory, question) -> str:\n",
    "        # good standard\n",
    "    prompt = F\"\"\"Task: all of the facts and rules are ture. Based on the provided Facts and Rules, please answer: is the Statement true or false?\n",
    "Example Theory: Dave is big. Dave is blue. Dave is furry. Dave is nice. Dave is rough. Dave is round. Dave is white. If Dave is blue and Dave is not furry then Dave is white. If someone is round and not blue then they are nice.\n",
    "Example Query: Dave is nice.\n",
    "Example Answer: True\n",
    "Facts and Rules: {theory}\n",
    "Statements: {question}\n",
    "Your Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good. If Bob is good then Bob is nice.'\n",
    "query='Bob is nice.'\n",
    "prompt = proofwriter_get_prompt(facts, query)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
