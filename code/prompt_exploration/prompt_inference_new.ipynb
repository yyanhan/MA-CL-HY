{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "import code\n",
    "import code_ablation\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question:str) -> str:\n",
    "    sequences = pipeline(\n",
    "        question,\n",
    "        do_sample=False,\n",
    "        # top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=400,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        return seq['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_2_str_num(facts:list) -> str:\n",
    "    res = \"\"\n",
    "    for index, fact in enumerate(facts):\n",
    "        res += f\"{index+1}. {fact}\\n\"\n",
    "    return res\n",
    "def list_2_str(facts:list) -> str:\n",
    "    res = \"\"\n",
    "    for index, fact in enumerate(facts):\n",
    "        res += f\"{fact}\\n\"\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['123']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"123\".split('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_extractor_inference(prompt, result):\n",
    "    result = result.strip()\n",
    "    if prompt in result:\n",
    "        result = result.replace(prompt, \"\")\n",
    "    if '</Answer>' in result:\n",
    "        result = result.split('</Answer>')[0].strip()\n",
    "    result_lines = result.split('\\n')\n",
    "    answer_label_line = result_lines[0]\n",
    "    if len(result_lines) > 1:\n",
    "        answer_conclusion_line = result_lines[1]\n",
    "    else:\n",
    "        answer_conclusion_line = ''\n",
    "\n",
    "    if 'yes' in answer_label_line.lower() and 'no' not in answer_label_line.lower():\n",
    "        answer_label = 'yes'\n",
    "    elif 'yes' not in answer_label_line.lower() and 'no' in answer_label_line.lower():\n",
    "        answer_label = 'no'\n",
    "    else:\n",
    "        answer_label = 'error'\n",
    "\n",
    "    answer_conclusion_after_produce = answer_conclusion_line.split('Produce:')\n",
    "    if len(answer_conclusion_after_produce) > 1:\n",
    "        answer_conclusion = answer_conclusion_after_produce[1].strip()\n",
    "    else:\n",
    "        answer_conclusion = answer_conclusion_after_produce[0]\n",
    "    \n",
    "    return answer_label, [answer_conclusion], answer_conclusion_line\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def evaluate_dev(prompt_answer_dict_list:list):\n",
    "\n",
    "    p_acc   = 0\n",
    "    p_acc_T = 0\n",
    "    p_acc_F = 0\n",
    "    c_acc   = 0\n",
    "    c_acc_T = 0\n",
    "    c_acc_F = 0\n",
    "    b_acc   = 0\n",
    "    b_acc_T = 0\n",
    "    b_acc_F = 0\n",
    "    n_err   = 0\n",
    "    n       = 0\n",
    "    num_NOTHING = 0\n",
    "    num_NOTHING_T = 0\n",
    "    num_NOTHING_F = 0\n",
    "\n",
    "    print(f\"[nr] [pred] [conc]; [gold] [conc] [full]\")\n",
    "    for prompt_answer in prompt_answer_dict_list:\n",
    "            prompt      = prompt_answer['prompt']\n",
    "            gold_answer = prompt_answer['answer']\n",
    "            conclusion = prompt_answer['conclusion']\n",
    "            correct_pred = False\n",
    "            correct_conc = False\n",
    "            error   = False\n",
    "\n",
    "            answer  = ask(prompt)\n",
    "            answer_label, answer_conclusion, answer_full = result_extractor_inference(prompt, answer)\n",
    "            if answer_label == 'error':\n",
    "                error = True\n",
    "            elif answer_label == gold_answer:\n",
    "                correct_pred = True\n",
    "            if answer_conclusion == conclusion:\n",
    "                correct_conc = True\n",
    "\n",
    "            # print(f\"[{n}] [{'correct' if correct_pred else 'error' if error else 'wrong'}] [{'correct' if correct_conc else 'wrong'}] [{gold_answer}/{answer_label}] [{conclusion}/{answer_conclusion}] {answer_full}\")\n",
    "            # compare with True Answer\n",
    "            n += 1\n",
    "            if answer_conclusion == 'NOTHING':\n",
    "                num_NOTHING += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    num_NOTHING_T += 1\n",
    "                elif gold_answer == 'no':\n",
    "                    num_NOTHING_F += 1\n",
    "\n",
    "            if error:\n",
    "                 n_err += 1\n",
    "            if correct_pred:\n",
    "                p_acc += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    p_acc_T += 1\n",
    "                elif gold_answer == 'no':\n",
    "                    p_acc_F += 1\n",
    "            \n",
    "            if correct_conc:\n",
    "                c_acc += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    c_acc_T += 1\n",
    "                elif gold_answer == 'False':\n",
    "                    c_acc_F += 1\n",
    "            \n",
    "            if correct_pred and correct_conc:\n",
    "                b_acc += 1\n",
    "                if gold_answer == 'yes':\n",
    "                    b_acc_T += 1\n",
    "                elif gold_answer == 'no':\n",
    "                    b_acc_F += 1\n",
    "\n",
    "            \n",
    "\n",
    "    print(f\"\"\"Pred_Acc*:\\t{p_acc}/100, \n",
    "Pred_TAcc*:\\t{p_acc_T}/50, {p_acc_T*2}%\n",
    "Pred_FAcc*:\\t{p_acc_F}/50, {p_acc_F*2}%\n",
    "\n",
    "Conc_Acc:\\t{c_acc}/100,\n",
    "Conc_TAcc*:\\t{c_acc_T}/50, {c_acc_T*2}%\n",
    "Conc_FAcc:\\t{c_acc_F}/50, {c_acc_F*2}%\n",
    "\n",
    "Both_Acc:\\t{b_acc}/100,\n",
    "Both_TAcc:\\t{b_acc_T}/50, {b_acc_T*2}%\n",
    "Both_FAcc:\\t{b_acc_F}/50, {b_acc_F*2}%\n",
    "\n",
    "num_NOTHING:\\t{num_NOTHING}/100,\n",
    "num_NOTHING_T:\\t{num_NOTHING_T}/50, {num_NOTHING_T*2}%\n",
    "num_NOTHING_F:\\t{num_NOTHING_F}/50, {num_NOTHING_F*2}%\n",
    "\n",
    "Err:\\t{n_err}/100\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_extractor_inference_multi(prompt, result):\n",
    "    result = result.strip()\n",
    "    if prompt in result:\n",
    "        result = result.replace(prompt, \"\")\n",
    "    if '</Answer>' in result:\n",
    "        result = result.split('</Answer>')[0].strip()\n",
    "    result_lines = result.split('\\n')\n",
    "    result_line = result_lines[0]\n",
    "    results = result_line.split(';')\n",
    "    res = []\n",
    "    for result in results:\n",
    "        result = result.strip()\n",
    "        if '. ' in result:  # 1. sentence\n",
    "            result = result.split('.')[1]\n",
    "            result = result.strip()\n",
    "        elif '.' in result: # sentence.\n",
    "            result = result.split('.')[0]\n",
    "            result = result.strip()\n",
    "        res.append(result)\n",
    "    return \"\", res, result_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Over all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. instruction satisfication\n",
    "2. example\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes' and answer what does it produce. if no, say 'no', and answer answer 'NOTHING'.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "</Example>\n",
      "Facts: Bob is good\n",
      "Rule: If Bob is good then Bob is nice,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer: no\n",
      "Produce: NOTHING\n",
      "</Answer>\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Facts: Sarah is tall\n",
      "Rule: If Sarah is tall then Sarah is happy.\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "2. Facts: John is old\n",
      "Rule: If John is old then John is grumpy.\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "3. Facts: Lisa is thin\n",
      "Rule: If Lisa is thin then Lisa is healthy.\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "4. Facts: Michael is smart\n",
      "Rule: If Michael is smart then Michael is successful.\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "5. Facts: Emily is funny\n",
      "Rule: If Emily is funny then Emily is popular.\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "Please answer the questions and I will provide you with the next set of facts and rules.\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
    "If yes, say 'yes' and answer what does it produce. if no, say 'no', and answer answer 'NOTHING'.\n",
    "<Example>\n",
    "Facts: Erin is round\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: yes, \n",
    "Procude: Erin is white\n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str},\n",
    "Please answer with the following format:\n",
    "Answer: [yes or no]\n",
    "Produce: [your answer here]\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good'\n",
    "rules='If Bob is good then Bob is nice'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes' and answer what does it produce. if no, say 'no', and answer answer 'NOTHING'.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "</Example>\n",
      "Facts: 1. the rabbit is round\n",
      "2. the rabbit needs the squirrel\n",
      "\n",
      "Rule: the rabbit is rough,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer:\n",
      "[nr] [pred] [conc]; [gold] [conc] [full]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [wrong] [wrong] [yes/no] [the rabbit is rough/NOTHING] Produce: NOTHING\n",
      "[1] [wrong] [wrong] [yes/no] [the cat chases the bear/NOTHING] Produce: NOTHING\n",
      "[2] [correct] [correct] [yes/yes] [the bear likes the cat/the bear likes the cat] Produce: the bear likes the cat\n",
      "[3] [correct] [wrong] [yes/yes] [Bob is kind/NOTHING] Produce: NOTHING\n",
      "[4] [correct] [correct] [yes/yes] [the cat visits the dog/the cat visits the dog] Produce: the cat visits the dog\n",
      "[5] [wrong] [wrong] [yes/no] [the bear sees the dog/NOTHING] Produce: NOTHING\n",
      "[6] [wrong] [wrong] [yes/no] [Bob is green/NOTHING] Produce: NOTHING\n",
      "[7] [wrong] [wrong] [yes/no] [the mouse visits the lion/NOTHING] Produce: NOTHING\n",
      "[8] [wrong] [wrong] [yes/no] [the dog is round/NOTHING] Produce: NOTHING\n",
      "[9] [wrong] [wrong] [yes/no] [the tiger eats the squirrel/NOTHING] Produce: NOTHING\n",
      "[10] [wrong] [wrong] [yes/no] [Bob is blue/NOTHING] Produce: NOTHING\n",
      "[11] [wrong] [wrong] [yes/no] [the lion is rough/NOTHING] Produce: NOTHING\n",
      "[12] [wrong] [wrong] [yes/no] [Bob is big/NOTHING] Produce: NOTHING\n",
      "[13] [wrong] [wrong] [yes/no] [the bald eagle needs the rabbit/NOTHING] Produce: NOTHING\n",
      "[14] [correct] [correct] [yes/yes] [the rabbit eats the tiger/the rabbit eats the tiger] Produce: the rabbit eats the tiger\n",
      "[15] [wrong] [wrong] [yes/no] [the dog is blue/NOTHING] Produce: NOTHING\n",
      "[16] [wrong] [wrong] [yes/no] [Bob is blue/NOTHING] Produce: NOTHING\n",
      "[17] [wrong] [wrong] [yes/no] [Bob is blue/NOTHING] Produce: NOTHING\n",
      "[18] [wrong] [wrong] [yes/no] [Bob is nice/NOTHING] Produce: NOTHING\n",
      "[19] [correct] [correct] [yes/yes] [the dog chases the lion/the dog chases the lion] Produce: the dog chases the lion\n",
      "[20] [wrong] [wrong] [yes/no] [the lion is green/NOTHING] Produce: NOTHING\n",
      "[21] [wrong] [wrong] [yes/no] [the dog is blue/NOTHING] Produce: NOTHING\n",
      "[22] [wrong] [wrong] [yes/no] [the bald eagle eats the tiger/NOTHING] Produce: NOTHING\n",
      "[23] [wrong] [wrong] [yes/no] [the dog eats the mouse/NOTHING] Produce: NOTHING\n",
      "[24] [wrong] [wrong] [yes/no] [the bald eagle needs the mouse/NOTHING] Produce: NOTHING\n",
      "[25] [correct] [wrong] [yes/yes] [the squirrel likes the bear/the squirrel likes Bob] Produce: the squirrel likes Bob\n",
      "[26] [wrong] [wrong] [yes/no] [Bob like the squirrel/NOTHING] Produce: NOTHING\n",
      "[27] [wrong] [wrong] [yes/no] [the squirrel needs the bear/NOTHING] Produce: NOTHING\n",
      "[28] [wrong] [wrong] [yes/no] [the dog is young/NOTHING] Produce: NOTHING\n",
      "[29] [wrong] [wrong] [yes/no] [the dog is furry/NOTHING] Produce: NOTHING\n",
      "[30] [correct] [correct] [yes/yes] [Gary is kind/Gary is kind] Produce: Gary is kind\n",
      "[31] [wrong] [wrong] [yes/no] [the dog is quiet/NOTHING] Produce: NOTHING\n",
      "[32] [wrong] [wrong] [yes/no] [the dog is cold/NOTHING] Produce: NOTHING\n",
      "[33] [correct] [wrong] [yes/yes] [the squirrel is big/NOTHING] Produce: NOTHING\n",
      "[34] [wrong] [wrong] [yes/no] [the squirrel is red/NOTHING] Produce: NOTHING\n",
      "[35] [wrong] [wrong] [yes/no] [Bob is quiet/NOTHING] Produce: NOTHING\n",
      "[36] [wrong] [wrong] [yes/no] [Bob is white/NOTHING] Produce: NOTHING\n",
      "[37] [wrong] [wrong] [yes/no] [Dave is round/NOTHING] Produce: NOTHING\n",
      "[38] [wrong] [wrong] [yes/no] [Dave is big/NOTHING] Produce: NOTHING\n",
      "[39] [wrong] [wrong] [yes/no] [Charlie is nice/NOTHING] Produce: NOTHING\n",
      "[40] [wrong] [wrong] [yes/no] [Bob is white/NOTHING] Produce: NOTHING\n",
      "[41] [wrong] [wrong] [yes/no] [Bob is cold/NOTHING] Produce: NOTHING\n",
      "[42] [wrong] [wrong] [yes/no] [the dog is cold/NOTHING] Produce: NOTHING\n",
      "[43] [wrong] [wrong] [yes/no] [Bob is quiet/NOTHING] Produce: NOTHING\n",
      "[44] [wrong] [wrong] [yes/no] [the squirrel is green/NOTHING] Produce: NOTHING\n",
      "[45] [wrong] [wrong] [yes/no] [the dog chases the mouse/NOTHING] Produce: NOTHING\n",
      "[46] [wrong] [wrong] [yes/no] [the dog needs the cat/NOTHING] Produce: NOTHING\n",
      "[47] [correct] [wrong] [yes/yes] [the dog needs the rabbit/the dog visits the rabbit] Produce: the dog visits the rabbit\n",
      "[48] [wrong] [wrong] [yes/no] [the rabbit needs the dog/NOTHING] Produce: NOTHING\n",
      "[49] [wrong] [wrong] [yes/no] [the rabbit visits the dog/NOTHING] Produce: NOTHING\n",
      "[50] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[51] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[52] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[53] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[54] [error] [correct] [no/error] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[55] [error] [correct] [no/error] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[56] [error] [correct] [no/error] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[57] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[58] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[59] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[60] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[61] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[62] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[63] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[64] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[65] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[66] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[67] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[68] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[69] [error] [correct] [no/error] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[70] [error] [correct] [no/error] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[71] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[72] [error] [correct] [no/error] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[73] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[74] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[75] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[76] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[77] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[78] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[79] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[80] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[81] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[82] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[83] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[84] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[85] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[86] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[87] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[88] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[89] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[90] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[91] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[92] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[93] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[94] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[95] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[96] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "[97] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[98] [correct] [correct] [no/no] [NOTHING/NOTHING] Produce: NOTHING\n",
      "[99] [error] [wrong] [no/error] [NOTHING/[your answer here]] Produce: [your answer here]\n",
      "Pred_Acc:\t45/100, \n",
      "Pred_TAcc:\t9/50, 18%\n",
      "Pred_FAcc:\t36/50, 72%\n",
      "\n",
      "Conc_Acc:\t47/100,\n",
      "Conc_TAcc:\t5/50, 10%\n",
      "Conc_FAcc:\t0/50, 0%\n",
      "\n",
      "Both_Acc:\t41/100,\n",
      "Both_TAcc:\t5/50, 10%\n",
      "Both_FAcc:\t36/50, 72%\n",
      "\n",
      "num_NOTHING:\t85/100,\n",
      "num_NOTHING_T:\t43/50, 86%\n",
      "num_NOTHING_F:\t42/50, 84%\n",
      "\n",
      "Err:\t14/100\n"
     ]
    }
   ],
   "source": [
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"./CWA_rules_100_TF.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': ablation_inference_prompt_formulate_nl(\n",
    "                list_2_str_num(data['conditions']),\n",
    "                data['conclusion']), \n",
    "        'answer':data['answer'],\n",
    "        'conclusion':data['conclusion'],\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Conclusion Without NOTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes' and answer what does it produce. if no, say 'no'.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "</Example>\n",
      "Facts: Bob is good\n",
      "Rule: If Bob is good then Bob is nice,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer: yes\n",
      "Produce: Bob is nice\n",
      "</Answer>\n",
      "Please answer the next question.\n",
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes' and answer what does it produce. if no, say 'no'.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "</Example>\n",
      "Facts: 1. the rabbit is round\n",
      "2. the rabbit needs the squirrel\n",
      "\n",
      "Rule: the rabbit is rough,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer:\n",
      "[nr] [pred] [conc]; [gold] [conc] [full]\n",
      "Pred_Acc:\t57/100, \n",
      "Pred_TAcc:\t7/50, 14%\n",
      "Pred_FAcc:\t50/50, 100%\n",
      "\n",
      "Conc_Acc:\t5/100,\n",
      "Conc_TAcc:\t5/50, 10%\n",
      "Conc_FAcc:\t0/50, 0%\n",
      "\n",
      "Both_Acc:\t5/100,\n",
      "Both_TAcc:\t5/50, 10%\n",
      "Both_FAcc:\t0/50, 0%\n",
      "\n",
      "num_NOTHING:\t0/100,\n",
      "num_NOTHING_T:\t0/50, 0%\n",
      "num_NOTHING_F:\t0/50, 0%\n",
      "\n",
      "Err:\t0/100\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
    "If yes, say 'yes' and answer what does it produce. if no, say 'no'.\n",
    "<Example>\n",
    "Facts: Erin is round\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: yes, \n",
    "Procude: Erin is white\n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str},\n",
    "Please answer with the following format:\n",
    "Answer: [yes or no]\n",
    "Produce: [your answer here]\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good'\n",
    "rules='If Bob is good then Bob is nice'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"./CWA_rules_100_TF.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': ablation_inference_prompt_formulate_nl(\n",
    "                list_2_str_num(data['conditions']),\n",
    "                data['conclusion']), \n",
    "        'answer':data['answer'],\n",
    "        'conclusion':data['conclusion'],\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Conclusion\n",
    "\n",
    "If yes, say 'yes', if no, say 'no',\n",
    "If it is satisfied, please answer what does it produce, else answer 'NOTHING'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce, else answer 'NOTHING'.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "</Example>\n",
      "Facts: Bob is good\n",
      "Rule: If Bob is good then Bob is nice,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer: yes\n",
      "Produce: Bob is nice\n",
      "</Answer>\n",
      "Please answer the next question.\n",
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce, else answer 'NOTHING'.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "</Example>\n",
      "Facts: 1. the rabbit is round\n",
      "2. the rabbit needs the squirrel\n",
      "\n",
      "Rule: the rabbit is rough,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer:\n",
      "[nr] [pred] [conc]; [gold] [conc] [full]\n",
      "Pred_Acc:\t71/100, \n",
      "Pred_TAcc:\t21/50, 42%\n",
      "Pred_FAcc:\t50/50, 100%\n",
      "\n",
      "Conc_Acc:\t57/100,\n",
      "Conc_TAcc:\t7/50, 14%\n",
      "Conc_FAcc:\t0/50, 0%\n",
      "\n",
      "Both_Acc:\t57/100,\n",
      "Both_TAcc:\t7/50, 14%\n",
      "Both_FAcc:\t50/50, 100%\n",
      "\n",
      "num_NOTHING:\t89/100,\n",
      "num_NOTHING_T:\t39/50, 78%\n",
      "num_NOTHING_F:\t50/50, 100%\n",
      "\n",
      "Err:\t0/100\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
    "If yes, say 'yes', if no, say 'no',\n",
    "If it is satisfied, please answer what does it produce.\n",
    "<Example>\n",
    "Facts: Erin is round\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: yes, \n",
    "Procude: Erin is white\n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str},\n",
    "Please answer with the following format:\n",
    "Answer: [yes or no]\n",
    "Produce: [your answer here]\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good'\n",
    "rules='If Bob is good then Bob is nice'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"./CWA_rules_100_TF.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': ablation_inference_prompt_formulate_nl(\n",
    "                list_2_str_num(data['conditions']),\n",
    "                data['conclusion']), \n",
    "        'answer':data['answer'],\n",
    "        'conclusion':data['conclusion'],\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negatie Example. bad prediction False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "Facts: Erin is kind\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: no, \n",
      "Procude: NOTHING\n",
      "</Example>\n",
      "Facts: Bob is good\n",
      "Rule: If Bob is good then Bob is nice,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer: yes\n",
      "Produce: Bob is nice\n",
      "</Answer>\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Is Bob good?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "2. Is Bob nice?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "3. Is Erin round?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "4. Is Erin white?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "Please answer the questions and I will provide you with the next set of facts and rules.\n",
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "Facts: Erin is kind\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: no, \n",
      "Procude: NOTHING\n",
      "</Example>\n",
      "Facts: 1. the rabbit is round\n",
      "2. the rabbit needs the squirrel\n",
      "\n",
      "Rule: the rabbit is rough,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer:\n",
      "[nr] [pred] [conc]; [gold] [conc] [full]\n",
      "Pred_Acc:\t52/100, \n",
      "Pred_TAcc:\t50/50, 100%\n",
      "Pred_FAcc:\t2/50, 4%\n",
      "\n",
      "Conc_Acc:\t40/100,\n",
      "Conc_TAcc:\t38/50, 76%\n",
      "Conc_FAcc:\t0/50, 0%\n",
      "\n",
      "Both_Acc:\t40/100,\n",
      "Both_TAcc:\t38/50, 76%\n",
      "Both_FAcc:\t2/50, 4%\n",
      "\n",
      "num_NOTHING:\t2/100,\n",
      "num_NOTHING_T:\t0/50, 0%\n",
      "num_NOTHING_F:\t2/50, 4%\n",
      "\n",
      "Err:\t47/100\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
    "If yes, say 'yes', if no, say 'no',\n",
    "If it is satisfied, please answer what does it produce.\n",
    "<Example>\n",
    "Facts: Erin is round\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: yes, \n",
    "Procude: Erin is white\n",
    "Facts: Erin is kind\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: no, \n",
    "Procude: NOTHING\n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str},\n",
    "Please answer with the following format:\n",
    "Answer: [yes or no]\n",
    "Produce: [your answer here]\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good'\n",
    "rules='If Bob is good then Bob is nice'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"./CWA_rules_100_TF.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': ablation_inference_prompt_formulate_nl(\n",
    "                list_2_str_num(data['conditions']),\n",
    "                data['conclusion']), \n",
    "        'answer':data['answer'],\n",
    "        'conclusion':data['conclusion'],\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instruction if-then, bad prediction False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If a \"if <condition> then <conclusion>\" rule is satisfied, the <conclusion> will be produced.\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "Facts: Erin is kind\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: no, \n",
      "Procude: NOTHING\n",
      "</Example>\n",
      "Facts: Bob is good.\n",
      "Rule: If Bob is good then Bob is nice.Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer: yes\n",
      "Produce: Bob is nice.\n",
      "</Answer>\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Is Bob good?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "2. Is Erin round?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "3. Is Erin white?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "Please answer the questions and I will provide you with the next set of facts and rules.\n",
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If a \"if <condition> then <conclusion>\" rule is satisfied, the <conclusion> will be produced.\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, \n",
      "Procude: Erin is white\n",
      "Facts: Erin is kind\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: no, \n",
      "Procude: NOTHING\n",
      "</Example>\n",
      "Facts: 1. the rabbit is round\n",
      "2. the rabbit needs the squirrel\n",
      ".\n",
      "Rule: the rabbit is rough.Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer:\n",
      "[nr] [pred] [conc]; [gold] [conc] [full]\n",
      "Pred_Acc:\t50/100, \n",
      "Pred_TAcc:\t50/50, 100%\n",
      "Pred_FAcc:\t0/50, 0%\n",
      "\n",
      "Conc_Acc:\t23/100,\n",
      "Conc_TAcc:\t23/50, 46%\n",
      "Conc_FAcc:\t0/50, 0%\n",
      "\n",
      "Both_Acc:\t23/100,\n",
      "Both_TAcc:\t23/50, 46%\n",
      "Both_FAcc:\t0/50, 0%\n",
      "\n",
      "num_NOTHING:\t0/100,\n",
      "num_NOTHING_T:\t0/50, 0%\n",
      "num_NOTHING_F:\t0/50, 0%\n",
      "\n",
      "Err:\t50/100\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
    "If a \"if <condition> then <conclusion>\" rule is satisfied, the <conclusion> will be produced.\n",
    "If yes, say 'yes', if no, say 'no',\n",
    "If it is satisfied, please answer what does it produce.\n",
    "<Example>\n",
    "Facts: Erin is round\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: yes, \n",
    "Procude: Erin is white\n",
    "Facts: Erin is kind\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: no, \n",
    "Procude: NOTHING\n",
    "</Example>\n",
    "Facts: {facts_str}.\n",
    "Rule: {rules_str}.Please answer with the following format:\n",
    "Answer: [yes or no]\n",
    "Produce: [your answer here]\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good'\n",
    "rules='If Bob is good then Bob is nice'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"./CWA_rules_100_TF.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': ablation_inference_prompt_formulate_nl(\n",
    "                list_2_str_num(data['conditions']),\n",
    "                data['conclusion']), \n",
    "        'answer':data['answer'],\n",
    "        'conclusion':data['conclusion'],\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reason - good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, the condition can be found in facts.\n",
      "Procude: Erin is white\n",
      "Facts: Erin is kind\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: no, the condition can't be found in facts.\n",
      "Procude: NOTHING\n",
      "</Example>\n",
      "Facts: Bob is good\n",
      "Rule: If Bob is good then Bob is nice,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer: yes\n",
      "Produce: Bob is nice\n",
      "</Answer>\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Is Bob round?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "2. Is Bob white?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Is Erin round?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "2. Is Erin white?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Is Sarah round?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "2. Is Sarah white?\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "\n",
      "Please answer the following questions:\n",
      "\n",
      "1. Is John round?\n",
      "Answer: [yes or no]\n",
      "Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
      "If yes, say 'yes', if no, say 'no',\n",
      "If it is satisfied, please answer what does it produce.\n",
      "<Example>\n",
      "Facts: Erin is round\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: yes, the condition can be found in facts.\n",
      "Procude: Erin is white\n",
      "Facts: Erin is kind\n",
      "Rule: If Erin is round then Erin is white.\n",
      "Answer: no, the condition can't be found in facts.\n",
      "Procude: NOTHING\n",
      "</Example>\n",
      "Facts: 1. the rabbit is round\n",
      "2. the rabbit needs the squirrel\n",
      "\n",
      "Rule: the rabbit is rough,\n",
      "Please answer with the following format:\n",
      "Answer: [yes or no]\n",
      "Produce: [your answer here]\n",
      "<Answer>\n",
      "Answer:\n",
      "[nr] [pred] [conc]; [gold] [conc] [full]\n",
      "Pred_Acc*:\t67/100, \n",
      "Pred_TAcc*:\t50/50, 100%\n",
      "Pred_FAcc*:\t17/50, 34%\n",
      "\n",
      "Conc_Acc:\t54/100,\n",
      "Conc_TAcc*:\t41/50, 82%\n",
      "Conc_FAcc:\t0/50, 0%\n",
      "\n",
      "Both_Acc:\t53/100,\n",
      "Both_TAcc:\t41/50, 82%\n",
      "Both_FAcc:\t12/50, 24%\n",
      "\n",
      "num_NOTHING:\t13/100,\n",
      "num_NOTHING_T:\t0/50, 0%\n",
      "num_NOTHING_F:\t13/50, 26%\n",
      "\n",
      "Err:\t0/100\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer whether the following rule is satisfied under the provided given Facts?\n",
    "If yes, say 'yes', if no, say 'no',\n",
    "If it is satisfied, please answer what does it produce.\n",
    "<Example>\n",
    "Facts: Erin is round\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: yes, the condition can be found in facts.\n",
    "Procude: Erin is white\n",
    "Facts: Erin is kind\n",
    "Rule: If Erin is round then Erin is white.\n",
    "Answer: no, the condition can't be found in facts.\n",
    "Procude: NOTHING\n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str},\n",
    "Please answer with the following format:\n",
    "Answer: [yes or no]\n",
    "Produce: [your answer here]\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good'\n",
    "rules='If Bob is good then Bob is nice'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n",
    "\n",
    "Dataset.cleanup_cache_files\n",
    "path_dev = \"./CWA_rules_100_TF.jsonl\"\n",
    "dataset = load_dataset('json', data_files=path_dev)\n",
    "\n",
    "prompt_answer_dict_list = [\n",
    "    {\n",
    "        'prompt': ablation_inference_prompt_formulate_nl(\n",
    "                list_2_str_num(data['conditions']),\n",
    "                data['conclusion']), \n",
    "        'answer':data['answer'],\n",
    "        'conclusion':data['conclusion'],\n",
    "    } for data in dataset['train']]\n",
    "print(prompt_answer_dict_list[0]['prompt'])\n",
    "evaluate_dev(prompt_answer_dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer what can be produce from the rules with given facts?\n",
      "<Example>\n",
      "Facts: Erin is round, Erin is nice\n",
      "Rule: If Erin is round then Erin is white. If Erin is nice then Erin is cute.\n",
      "Answer: Erin is white. Erin is cute.\n",
      "Facts: Bob is kind; Bob is rude; Bob is rough;\n",
      "Rule: If Bob is kind then Bob is blue; If Bob is rough, then Bob is nice; If Bob is cute then Bob is kind;\n",
      "Answer: Bob is blue; Bob is nice; \n",
      "</Example>\n",
      "Facts: Bob is good. Bob is cute.\n",
      "Rule: If Bob is good then Bob is nice. If Bob is cute then Bob is white. Please answer with the following format:\n",
      "Answer: [your answer here].\n",
      "<Answer>\n",
      "Answer: Bob is white.\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer what can be produce from the rules with given facts?\n",
    "<Example>\n",
    "Facts: Erin is round, Erin is nice\n",
    "Rule: If Erin is round then Erin is white. If Erin is nice then Erin is cute.\n",
    "Answer: Erin is white. Erin is cute.\n",
    "Facts: Bob is kind; Bob is rude; Bob is rough;\n",
    "Rule: If Bob is kind then Bob is blue; If Bob is rough, then Bob is nice; If Bob is cute then Bob is kind;\n",
    "Answer: Bob is blue; Bob is nice; \n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str} Please answer with the following format:\n",
    "Answer: [your answer here].\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='Bob is good. Bob is cute.'\n",
    "rules='If Bob is good then Bob is nice. If Bob is cute then Bob is white.'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer what can be produce from the rules with given facts?\n",
      "<Example>\n",
      "Facts: 1. Erin is round; 2. Erin is nice;\n",
      "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
      "Answer: 1. Erin is white; 2. Erin is cute;\n",
      "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
      "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
      "Answer: 1. Bob is blue; 2. Bob is nice; \n",
      "</Example>\n",
      "Facts: 1. Bob is good; 2. Bob is cute.\n",
      "Rule: 1. If Bob is good then Bob is nice; 2. If Bob is cute then Bob is white. Please answer with the following format:\n",
      "Answer: [your answer here].\n",
      "<Answer>\n",
      "Answer: Bob is nice; Bob is white.\n",
      "</Answer>\n",
      "\n",
      "Please answer the question with the given format.\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer what can be produce from the rules with given facts?\n",
    "<Example>\n",
    "Facts: 1. Erin is round; 2. Erin is nice;\n",
    "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
    "Answer: 1. Erin is white; 2. Erin is cute;\n",
    "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
    "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
    "Answer: 1. Bob is blue; 2. Bob is nice; \n",
    "</Example>\n",
    "Facts: {facts_str}.\n",
    "Rule: {rules_str}. Please answer with the following format:\n",
    "Answer: [your answer here].\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts='1. Bob is good; 2. Bob is cute.'\n",
    "rules='1. If Bob is good then Bob is nice; 2. If Bob is cute then Bob is white.'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghn/env_py/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer what can be produce from the rules with given facts?\n",
      "<Example>\n",
      "Facts: 1. Erin is round; 2. Erin is nice;\n",
      "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
      "Answer: 1. Erin is white; 2. Erin is cute;\n",
      "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
      "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
      "Answer: 1. Bob is blue; 2. Bob is nice; \n",
      "</Example>\n",
      "Facts: 1. Bob is good; \n",
      "2. Bob is cute.\n",
      "Rule: 1. If Bob is good then Bob is nice; \n",
      "2. If Bob is cute then Bob is white. \n",
      "Please answer with the following format:\n",
      "Answer: [your answer here].\n",
      "<Answer>\n",
      "Answer: Erin is white.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is blue.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is nice.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is white.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Erin is cute.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is nice.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is white.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Erin is cute.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is blue.\n",
      "</Answer>\n",
      "<Answer>\n",
      "Answer: Bob is nice.\n",
      "</Answer\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer what can be produce from the rules with given facts?\n",
    "<Example>\n",
    "Facts: 1. Erin is round; 2. Erin is nice;\n",
    "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
    "Answer: 1. Erin is white; 2. Erin is cute;\n",
    "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
    "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
    "Answer: 1. Bob is blue; 2. Bob is nice; \n",
    "</Example>\n",
    "Facts: {facts_str}\n",
    "Rule: {rules_str} \n",
    "Please answer with the following format:\n",
    "Answer: [your answer here].\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts=\"\"\"1. Bob is good; \n",
    "2. Bob is cute.\"\"\"\n",
    "rules=\"\"\"1. If Bob is good then Bob is nice; \n",
    "2. If Bob is cute then Bob is white.\"\"\"\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No facts (Bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer what can be produce from the rules with given facts?\n",
      "<Example>\n",
      "Facts: 1. Erin is round; 2. Erin is nice;\n",
      "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
      "Answer: 1. Erin is white; 2. Erin is cute;\n",
      "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
      "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
      "Answer: 1. Bob is blue; 2. Bob is nice; \n",
      "</Example>\n",
      "Facts: .\n",
      "Rule: 1. If Bob is good then Bob is nice; 2. If Bob is cute then Bob is white.. Please answer with the following format:\n",
      "Answer: [your answer here].\n",
      "<Answer>\n",
      "Answer: Bob is nice; Bob is white.\n",
      "</Answer>\n",
      "\n",
      "Answer: Bob is nice; Bob is white.\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer what can be produce from the rules with given facts?\n",
    "<Example>\n",
    "Facts: 1. Erin is round; 2. Erin is nice;\n",
    "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
    "Answer: 1. Erin is white; 2. Erin is cute;\n",
    "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
    "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
    "Answer: 1. Bob is blue; 2. Bob is nice; \n",
    "</Example>\n",
    "Facts: {facts_str}.\n",
    "Rule: {rules_str}. Please answer with the following format:\n",
    "Answer: [your answer here].\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts=''\n",
    "rules='1. If Bob is good then Bob is nice; 2. If Bob is cute then Bob is white.'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No facts with NOTHING Example (Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: please answer what can be produce from the rules with given facts?\n",
      "<Example>\n",
      "Facts: 1. Erin is round; 2. Erin is nice;\n",
      "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
      "Answer: 1. Erin is white; 2. Erin is cute;\n",
      "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
      "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
      "Answer: 1. Bob is blue; 2. Bob is nice; \n",
      "Facts: 1. Bob is kind; 2. Bob is rude;\n",
      "Rule: 1. If Bob is pretty then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
      "Answer: NOTHING\n",
      "</Example>\n",
      "Facts: .\n",
      "Rule: 1. If Bob is good then Bob is nice; 2. If Bob is cute then Bob is white.. Please answer with the following format:\n",
      "Answer: [your answer here].\n",
      "<Answer>\n",
      "Answer: 1. Bob is nice; 2. Bob is white.\n"
     ]
    }
   ],
   "source": [
    "def ablation_inference_prompt_formulate_nl(facts_str:str, rules_str:str):\n",
    "    prompt = f\"\"\"Task: please answer what can be produce from the rules with given facts?\n",
    "<Example>\n",
    "Facts: 1. Erin is round; 2. Erin is nice;\n",
    "Rule: 1. If Erin is round then Erin is white; 2. If Erin is nice then Erin is cute;\n",
    "Answer: 1. Erin is white; 2. Erin is cute;\n",
    "Facts: 1. Bob is kind; 2. Bob is rude; 3. Bob is rough;\n",
    "Rule: 1. If Bob is kind then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
    "Answer: 1. Bob is blue; 2. Bob is nice; \n",
    "Facts: 1. Bob is kind; 2. Bob is rude;\n",
    "Rule: 1. If Bob is pretty then Bob is blue; 2. If Bob is rough, then Bob is nice; 3. If Bob is cute then Bob is kind;\n",
    "Answer: NOTHING\n",
    "</Example>\n",
    "Facts: {facts_str}.\n",
    "Rule: {rules_str}. Please answer with the following format:\n",
    "Answer: [your answer here].\n",
    "<Answer>\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "facts=''\n",
    "rules='1. If Bob is good then Bob is nice; 2. If Bob is cute then Bob is white.'\n",
    "prompt = ablation_inference_prompt_formulate_nl(facts, rules)\n",
    "answer = ask(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', ['Bob is nice', 'Bob is white'], 'Bob is nice; Bob is white.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_extractor_inference_multi(prompt, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
